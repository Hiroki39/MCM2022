{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SEQUENCE_LENGTH = 30\n",
    "HIDDEN_LAYER_SIZE = 50\n",
    "LEARNING_RATE = 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"Data/preprocessed_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Value</th>\n",
       "      <th>USD (PM)</th>\n",
       "      <th>Bitcoin % Change</th>\n",
       "      <th>Gold % Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9/11/16</td>\n",
       "      <td>621.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9/12/16</td>\n",
       "      <td>609.67</td>\n",
       "      <td>1324.60</td>\n",
       "      <td>-0.019271</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9/13/16</td>\n",
       "      <td>610.92</td>\n",
       "      <td>1323.65</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>-0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9/14/16</td>\n",
       "      <td>608.82</td>\n",
       "      <td>1321.75</td>\n",
       "      <td>-0.003437</td>\n",
       "      <td>-0.001435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9/15/16</td>\n",
       "      <td>610.38</td>\n",
       "      <td>1310.80</td>\n",
       "      <td>0.002562</td>\n",
       "      <td>-0.008284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>1821</td>\n",
       "      <td>9/6/21</td>\n",
       "      <td>51769.06</td>\n",
       "      <td>1821.60</td>\n",
       "      <td>0.036472</td>\n",
       "      <td>-0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>1822</td>\n",
       "      <td>9/7/21</td>\n",
       "      <td>52677.40</td>\n",
       "      <td>1802.15</td>\n",
       "      <td>0.017546</td>\n",
       "      <td>-0.010677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>1823</td>\n",
       "      <td>9/8/21</td>\n",
       "      <td>46809.17</td>\n",
       "      <td>1786.00</td>\n",
       "      <td>-0.111399</td>\n",
       "      <td>-0.008962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>1824</td>\n",
       "      <td>9/9/21</td>\n",
       "      <td>46078.38</td>\n",
       "      <td>1788.25</td>\n",
       "      <td>-0.015612</td>\n",
       "      <td>0.001260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>1825</td>\n",
       "      <td>9/10/21</td>\n",
       "      <td>46368.69</td>\n",
       "      <td>1794.60</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.003551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1826 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     Date     Value  USD (PM)  Bitcoin % Change  Gold % Change\n",
       "0              0  9/11/16    621.65       NaN               NaN            NaN\n",
       "1              1  9/12/16    609.67   1324.60         -0.019271            NaN\n",
       "2              2  9/13/16    610.92   1323.65          0.002050      -0.000717\n",
       "3              3  9/14/16    608.82   1321.75         -0.003437      -0.001435\n",
       "4              4  9/15/16    610.38   1310.80          0.002562      -0.008284\n",
       "...          ...      ...       ...       ...               ...            ...\n",
       "1821        1821   9/6/21  51769.06   1821.60          0.036472      -0.000384\n",
       "1822        1822   9/7/21  52677.40   1802.15          0.017546      -0.010677\n",
       "1823        1823   9/8/21  46809.17   1786.00         -0.111399      -0.008962\n",
       "1824        1824   9/9/21  46078.38   1788.25         -0.015612       0.001260\n",
       "1825        1825  9/10/21  46368.69   1794.60          0.006300       0.003551\n",
       "\n",
       "[1826 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = len(df_all) // 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_all.drop(\"Date\", axis=1)\n",
    "# df_train, df_test = train_test_split(df_data, test_size=0.2)\n",
    "df_train = df_data.iloc[:-num_test, [2]].copy()\n",
    "df_test = df_data.iloc[-num_test - SEQUENCE_LENGTH:, [2]].copy()\n",
    "# df_train = df_data.iloc[num_test:, :1].copy()\n",
    "# df_test = df_data.iloc[:num_test + SEQUENCE_LENGTH:, :1].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(df_train)\n",
    "X_test = sc.transform(df_test)\n",
    "y_train = df_train[[\"Bitcoin % Change\"]].values\n",
    "y_test = df_test[[\"Bitcoin % Change\"]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y, sequence_length=SEQUENCE_LENGTH):\n",
    "        # self.features = features\n",
    "        # self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.y = torch.tensor(y).float().cuda()\n",
    "        self.X = torch.tensor(X).float().cuda()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0] - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.X[i:(i + self.sequence_length), :]\n",
    "        y = self.y[i + self.sequence_length, :]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SequenceDataset(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "test_dataset = SequenceDataset(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    sequence_length=SEQUENCE_LENGTH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = HIDDEN_LAYER_SIZE\n",
    "        self.lstm = nn.LSTM(input_size=SEQUENCE_LENGTH,\n",
    "                            hidden_size=self.hidden_layer_size,\n",
    "                            num_layers=2,\n",
    "                            dropout=0.5,\n",
    "                            batch_first=True)\n",
    "        # equivalent to Dense in keras\n",
    "        self.linear1 = nn.Linear(self.hidden_layer_size, self.hidden_layer_size)\n",
    "        self.linear2 = nn.Linear(self.hidden_layer_size, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(2, BATCH_SIZE, self.hidden_layer_size).\n",
    "                            cuda(),\n",
    "                            torch.zeros(2, BATCH_SIZE, self.hidden_layer_size).cuda())\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        _, (hn, _) = self.lstm(input_seq.view(\n",
    "            len(input_seq), 1, -1), self.hidden_cell)\n",
    "        # lstm_out, self.hidden_cell = self.lstm(input_seq, self.hidden_cell)\n",
    "        # predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        predictions = self.dropout(self.linear2(hn[-1]))\n",
    "        # predictions = self.dropout(self.linear2(predictions))\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "model.cuda()\n",
    "loss_function = nn.MSELoss()\n",
    "loss_function.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_num = 0\n",
    "\n",
    "    for X, y in data_loader:\n",
    "\n",
    "        # print('Training batch', batch_num)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model.hidden_cell = (torch.zeros(2, X.shape[0], model.hidden_layer_size).cuda(\n",
    "        ), torch.zeros(2, X.shape[0], model.hidden_layer_size).cuda())\n",
    "\n",
    "        predictions = model(X)\n",
    "\n",
    "        # print(predictions.shape, y.shape)\n",
    "\n",
    "        loss = criterion(predictions, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        batch_num += 1\n",
    "\n",
    "    return epoch_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for X, y in data_loader:\n",
    "\n",
    "            model.hidden_cell = (torch.zeros(2, X.shape[0], model.hidden_layer_size).cuda(\n",
    "            ), torch.zeros(2, X.shape[0], model.hidden_layer_size).cuda())\n",
    "\n",
    "            predictions = model(X)\n",
    "\n",
    "            loss = criterion(predictions, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0036920884\n",
      "\t Val. Loss: 0.0028477229\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0035282936\n",
      "\t Val. Loss: 0.0027858011\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0036595789\n",
      "\t Val. Loss: 0.0027256541\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0035851996\n",
      "\t Val. Loss: 0.0026687310\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0035363981\n",
      "\t Val. Loss: 0.0026131896\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0034009109\n",
      "\t Val. Loss: 0.0025619918\n",
      "Epoch: 07 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0032269018\n",
      "\t Val. Loss: 0.0025125302\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0030706317\n",
      "\t Val. Loss: 0.0024679234\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0030696963\n",
      "\t Val. Loss: 0.0024248890\n",
      "Epoch: 10 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0029860524\n",
      "\t Val. Loss: 0.0023849752\n",
      "Epoch: 11 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0030043529\n",
      "\t Val. Loss: 0.0023437899\n",
      "Epoch: 12 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0029510357\n",
      "\t Val. Loss: 0.0023047533\n",
      "Epoch: 13 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0027723982\n",
      "\t Val. Loss: 0.0022703609\n",
      "Epoch: 14 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0028298112\n",
      "\t Val. Loss: 0.0022355744\n",
      "Epoch: 15 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0027530202\n",
      "\t Val. Loss: 0.0022020116\n",
      "Epoch: 16 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0026113600\n",
      "\t Val. Loss: 0.0021719094\n",
      "Epoch: 17 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0025849229\n",
      "\t Val. Loss: 0.0021428613\n",
      "Epoch: 18 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0025375583\n",
      "\t Val. Loss: 0.0021135656\n",
      "Epoch: 19 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0024743208\n",
      "\t Val. Loss: 0.0020880199\n",
      "Epoch: 20 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0025079287\n",
      "\t Val. Loss: 0.0020629942\n",
      "Epoch: 21 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0023806826\n",
      "\t Val. Loss: 0.0020410467\n",
      "Epoch: 22 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0023382348\n",
      "\t Val. Loss: 0.0020208805\n",
      "Epoch: 23 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0023306258\n",
      "\t Val. Loss: 0.0020011676\n",
      "Epoch: 24 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0022986914\n",
      "\t Val. Loss: 0.0019828459\n",
      "Epoch: 25 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0022532769\n",
      "\t Val. Loss: 0.0019642222\n",
      "Epoch: 26 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0022172164\n",
      "\t Val. Loss: 0.0019477188\n",
      "Epoch: 27 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0022044939\n",
      "\t Val. Loss: 0.0019310487\n",
      "Epoch: 28 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0021425446\n",
      "\t Val. Loss: 0.0019159209\n",
      "Epoch: 29 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0021427343\n",
      "\t Val. Loss: 0.0019024537\n",
      "Epoch: 30 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0020854284\n",
      "\t Val. Loss: 0.0018891077\n",
      "Epoch: 31 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0020407839\n",
      "\t Val. Loss: 0.0018782903\n",
      "Epoch: 32 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0020873603\n",
      "\t Val. Loss: 0.0018673448\n",
      "Epoch: 33 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0020690533\n",
      "\t Val. Loss: 0.0018565928\n",
      "Epoch: 34 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0020254949\n",
      "\t Val. Loss: 0.0018459519\n",
      "Epoch: 35 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0020166730\n",
      "\t Val. Loss: 0.0018375944\n",
      "Epoch: 36 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0019891827\n",
      "\t Val. Loss: 0.0018294493\n",
      "Epoch: 37 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0019729245\n",
      "\t Val. Loss: 0.0018209528\n",
      "Epoch: 38 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0020030237\n",
      "\t Val. Loss: 0.0018134909\n",
      "Epoch: 39 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0019489038\n",
      "\t Val. Loss: 0.0018070423\n",
      "Epoch: 40 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0019417878\n",
      "\t Val. Loss: 0.0018014831\n",
      "Epoch: 41 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0019660977\n",
      "\t Val. Loss: 0.0017956747\n",
      "Epoch: 42 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0019812270\n",
      "\t Val. Loss: 0.0017881534\n",
      "Epoch: 43 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0019617928\n",
      "\t Val. Loss: 0.0017831517\n",
      "Epoch: 44 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0019168186\n",
      "\t Val. Loss: 0.0017774007\n",
      "Epoch: 45 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018696281\n",
      "\t Val. Loss: 0.0017726453\n",
      "Epoch: 46 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018932414\n",
      "\t Val. Loss: 0.0017685190\n",
      "Epoch: 47 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0019514982\n",
      "\t Val. Loss: 0.0017649928\n",
      "Epoch: 48 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0019553067\n",
      "\t Val. Loss: 0.0017616237\n",
      "Epoch: 49 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0019438931\n",
      "\t Val. Loss: 0.0017577234\n",
      "Epoch: 50 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018635193\n",
      "\t Val. Loss: 0.0017551491\n",
      "Epoch: 51 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0019156548\n",
      "\t Val. Loss: 0.0017531942\n",
      "Epoch: 52 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0019093779\n",
      "\t Val. Loss: 0.0017503210\n",
      "Epoch: 53 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0019055773\n",
      "\t Val. Loss: 0.0017482156\n",
      "Epoch: 54 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0019163420\n",
      "\t Val. Loss: 0.0017450729\n",
      "Epoch: 55 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018792420\n",
      "\t Val. Loss: 0.0017426911\n",
      "Epoch: 56 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018564442\n",
      "\t Val. Loss: 0.0017406293\n",
      "Epoch: 57 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018496372\n",
      "\t Val. Loss: 0.0017386630\n",
      "Epoch: 58 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018147272\n",
      "\t Val. Loss: 0.0017378012\n",
      "Epoch: 59 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018615409\n",
      "\t Val. Loss: 0.0017353989\n",
      "Epoch: 60 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018350212\n",
      "\t Val. Loss: 0.0017340508\n",
      "Epoch: 61 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018422067\n",
      "\t Val. Loss: 0.0017332989\n",
      "Epoch: 62 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018918900\n",
      "\t Val. Loss: 0.0017315964\n",
      "Epoch: 63 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018658596\n",
      "\t Val. Loss: 0.0017298087\n",
      "Epoch: 64 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018531434\n",
      "\t Val. Loss: 0.0017293601\n",
      "Epoch: 65 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018525661\n",
      "\t Val. Loss: 0.0017287975\n",
      "Epoch: 66 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018339603\n",
      "\t Val. Loss: 0.0017284065\n",
      "Epoch: 67 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018630280\n",
      "\t Val. Loss: 0.0017274152\n",
      "Epoch: 68 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018707358\n",
      "\t Val. Loss: 0.0017263769\n",
      "Epoch: 69 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018854360\n",
      "\t Val. Loss: 0.0017264172\n",
      "Epoch: 70 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018256394\n",
      "\t Val. Loss: 0.0017259059\n",
      "Epoch: 71 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018610894\n",
      "\t Val. Loss: 0.0017253701\n",
      "Epoch: 72 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018350910\n",
      "\t Val. Loss: 0.0017246810\n",
      "Epoch: 73 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018541766\n",
      "\t Val. Loss: 0.0017240472\n",
      "Epoch: 74 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018159780\n",
      "\t Val. Loss: 0.0017234583\n",
      "Epoch: 75 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018318704\n",
      "\t Val. Loss: 0.0017223199\n",
      "Epoch: 76 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018337084\n",
      "\t Val. Loss: 0.0017218586\n",
      "Epoch: 77 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018784716\n",
      "\t Val. Loss: 0.0017222033\n",
      "Epoch: 78 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018165334\n",
      "\t Val. Loss: 0.0017215763\n",
      "Epoch: 79 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018448918\n",
      "\t Val. Loss: 0.0017217331\n",
      "Epoch: 80 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018335137\n",
      "\t Val. Loss: 0.0017211926\n",
      "Epoch: 81 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018248680\n",
      "\t Val. Loss: 0.0017204688\n",
      "Epoch: 82 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018428169\n",
      "\t Val. Loss: 0.0017198633\n",
      "Epoch: 83 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018090568\n",
      "\t Val. Loss: 0.0017189090\n",
      "Epoch: 84 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018694962\n",
      "\t Val. Loss: 0.0017181759\n",
      "Epoch: 85 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018502654\n",
      "\t Val. Loss: 0.0017179886\n",
      "Epoch: 86 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018456864\n",
      "\t Val. Loss: 0.0017177676\n",
      "Epoch: 87 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018291113\n",
      "\t Val. Loss: 0.0017175656\n",
      "Epoch: 88 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018255193\n",
      "\t Val. Loss: 0.0017174767\n",
      "Epoch: 89 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018108336\n",
      "\t Val. Loss: 0.0017172962\n",
      "Epoch: 90 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018172610\n",
      "\t Val. Loss: 0.0017176399\n",
      "Epoch: 91 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018360524\n",
      "\t Val. Loss: 0.0017179284\n",
      "Epoch: 92 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017573227\n",
      "\t Val. Loss: 0.0017176133\n",
      "Epoch: 93 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018052591\n",
      "\t Val. Loss: 0.0017178788\n",
      "Epoch: 94 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018230770\n",
      "\t Val. Loss: 0.0017172685\n",
      "Epoch: 95 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017676049\n",
      "\t Val. Loss: 0.0017170076\n",
      "Epoch: 96 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018281640\n",
      "\t Val. Loss: 0.0017169766\n",
      "Epoch: 97 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018570035\n",
      "\t Val. Loss: 0.0017170582\n",
      "Epoch: 98 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018049133\n",
      "\t Val. Loss: 0.0017166465\n",
      "Epoch: 99 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017889256\n",
      "\t Val. Loss: 0.0017164687\n",
      "Epoch: 100 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018168390\n",
      "\t Val. Loss: 0.0017162962\n",
      "Epoch: 101 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018272440\n",
      "\t Val. Loss: 0.0017164441\n",
      "Epoch: 102 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018193737\n",
      "\t Val. Loss: 0.0017154211\n",
      "Epoch: 103 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017936696\n",
      "\t Val. Loss: 0.0017149572\n",
      "Epoch: 104 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018087214\n",
      "\t Val. Loss: 0.0017145741\n",
      "Epoch: 105 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018148270\n",
      "\t Val. Loss: 0.0017140744\n",
      "Epoch: 106 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018034164\n",
      "\t Val. Loss: 0.0017139676\n",
      "Epoch: 107 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017988006\n",
      "\t Val. Loss: 0.0017138792\n",
      "Epoch: 108 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018076135\n",
      "\t Val. Loss: 0.0017146786\n",
      "Epoch: 109 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018080387\n",
      "\t Val. Loss: 0.0017146093\n",
      "Epoch: 110 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017972892\n",
      "\t Val. Loss: 0.0017149021\n",
      "Epoch: 111 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018307051\n",
      "\t Val. Loss: 0.0017149885\n",
      "Epoch: 112 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018148818\n",
      "\t Val. Loss: 0.0017152383\n",
      "Epoch: 113 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018004773\n",
      "\t Val. Loss: 0.0017148521\n",
      "Epoch: 114 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018196993\n",
      "\t Val. Loss: 0.0017140591\n",
      "Epoch: 115 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018024551\n",
      "\t Val. Loss: 0.0017145712\n",
      "Epoch: 116 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018327545\n",
      "\t Val. Loss: 0.0017141948\n",
      "Epoch: 117 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018241545\n",
      "\t Val. Loss: 0.0017138730\n",
      "Epoch: 118 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017819904\n",
      "\t Val. Loss: 0.0017137912\n",
      "Epoch: 119 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018252881\n",
      "\t Val. Loss: 0.0017137280\n",
      "Epoch: 120 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017839920\n",
      "\t Val. Loss: 0.0017132760\n",
      "Epoch: 121 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018359519\n",
      "\t Val. Loss: 0.0017132242\n",
      "Epoch: 122 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017886547\n",
      "\t Val. Loss: 0.0017134106\n",
      "Epoch: 123 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017704354\n",
      "\t Val. Loss: 0.0017134568\n",
      "Epoch: 124 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018106768\n",
      "\t Val. Loss: 0.0017130894\n",
      "Epoch: 125 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017949637\n",
      "\t Val. Loss: 0.0017127777\n",
      "Epoch: 126 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017628432\n",
      "\t Val. Loss: 0.0017125655\n",
      "Epoch: 127 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018017514\n",
      "\t Val. Loss: 0.0017123649\n",
      "Epoch: 128 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017891817\n",
      "\t Val. Loss: 0.0017114717\n",
      "Epoch: 129 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017851966\n",
      "\t Val. Loss: 0.0017120314\n",
      "Epoch: 130 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017860576\n",
      "\t Val. Loss: 0.0017115546\n",
      "Epoch: 131 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018337231\n",
      "\t Val. Loss: 0.0017110554\n",
      "Epoch: 132 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018088391\n",
      "\t Val. Loss: 0.0017106435\n",
      "Epoch: 133 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018086747\n",
      "\t Val. Loss: 0.0017110430\n",
      "Epoch: 134 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018440744\n",
      "\t Val. Loss: 0.0017109291\n",
      "Epoch: 135 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017649184\n",
      "\t Val. Loss: 0.0017109644\n",
      "Epoch: 136 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017708734\n",
      "\t Val. Loss: 0.0017102612\n",
      "Epoch: 137 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018065090\n",
      "\t Val. Loss: 0.0017098179\n",
      "Epoch: 138 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017743401\n",
      "\t Val. Loss: 0.0017093759\n",
      "Epoch: 139 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017670360\n",
      "\t Val. Loss: 0.0017094698\n",
      "Epoch: 140 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018132077\n",
      "\t Val. Loss: 0.0017104128\n",
      "Epoch: 141 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017878666\n",
      "\t Val. Loss: 0.0017109354\n",
      "Epoch: 142 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017850083\n",
      "\t Val. Loss: 0.0017104048\n",
      "Epoch: 143 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018052037\n",
      "\t Val. Loss: 0.0017101461\n",
      "Epoch: 144 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018157744\n",
      "\t Val. Loss: 0.0017096064\n",
      "Epoch: 145 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017899229\n",
      "\t Val. Loss: 0.0017093119\n",
      "Epoch: 146 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018007020\n",
      "\t Val. Loss: 0.0017094747\n",
      "Epoch: 147 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017851957\n",
      "\t Val. Loss: 0.0017093941\n",
      "Epoch: 148 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017922133\n",
      "\t Val. Loss: 0.0017103922\n",
      "Epoch: 149 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017818977\n",
      "\t Val. Loss: 0.0017095940\n",
      "Epoch: 150 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017979801\n",
      "\t Val. Loss: 0.0017098783\n",
      "Epoch: 151 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017954314\n",
      "\t Val. Loss: 0.0017095552\n",
      "Epoch: 152 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017856609\n",
      "\t Val. Loss: 0.0017091952\n",
      "Epoch: 153 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017932053\n",
      "\t Val. Loss: 0.0017092622\n",
      "Epoch: 154 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017716362\n",
      "\t Val. Loss: 0.0017082001\n",
      "Epoch: 155 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017618798\n",
      "\t Val. Loss: 0.0017084045\n",
      "Epoch: 156 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017640073\n",
      "\t Val. Loss: 0.0017089764\n",
      "Epoch: 157 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017798141\n",
      "\t Val. Loss: 0.0017091812\n",
      "Epoch: 158 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017690543\n",
      "\t Val. Loss: 0.0017093542\n",
      "Epoch: 159 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017863583\n",
      "\t Val. Loss: 0.0017090875\n",
      "Epoch: 160 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017634575\n",
      "\t Val. Loss: 0.0017089220\n",
      "Epoch: 161 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017449131\n",
      "\t Val. Loss: 0.0017093892\n",
      "Epoch: 162 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018265392\n",
      "\t Val. Loss: 0.0017092688\n",
      "Epoch: 163 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017602007\n",
      "\t Val. Loss: 0.0017093881\n",
      "Epoch: 164 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017926395\n",
      "\t Val. Loss: 0.0017094685\n",
      "Epoch: 165 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017982869\n",
      "\t Val. Loss: 0.0017097065\n",
      "Epoch: 166 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017997195\n",
      "\t Val. Loss: 0.0017100273\n",
      "Epoch: 167 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017795427\n",
      "\t Val. Loss: 0.0017100883\n",
      "Epoch: 168 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018085573\n",
      "\t Val. Loss: 0.0017104761\n",
      "Epoch: 169 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017451832\n",
      "\t Val. Loss: 0.0017098836\n",
      "Epoch: 170 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018046036\n",
      "\t Val. Loss: 0.0017094524\n",
      "Epoch: 171 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017643187\n",
      "\t Val. Loss: 0.0017092671\n",
      "Epoch: 172 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017759574\n",
      "\t Val. Loss: 0.0017091654\n",
      "Epoch: 173 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017753598\n",
      "\t Val. Loss: 0.0017079498\n",
      "Epoch: 174 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017754465\n",
      "\t Val. Loss: 0.0017077955\n",
      "Epoch: 175 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017779096\n",
      "\t Val. Loss: 0.0017078033\n",
      "Epoch: 176 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017846789\n",
      "\t Val. Loss: 0.0017075215\n",
      "Epoch: 177 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017415426\n",
      "\t Val. Loss: 0.0017071978\n",
      "Epoch: 178 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017908069\n",
      "\t Val. Loss: 0.0017071729\n",
      "Epoch: 179 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017931875\n",
      "\t Val. Loss: 0.0017073108\n",
      "Epoch: 180 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017549400\n",
      "\t Val. Loss: 0.0017076331\n",
      "Epoch: 181 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017905379\n",
      "\t Val. Loss: 0.0017081990\n",
      "Epoch: 182 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017605537\n",
      "\t Val. Loss: 0.0017084361\n",
      "Epoch: 183 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017465304\n",
      "\t Val. Loss: 0.0017081123\n",
      "Epoch: 184 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017817888\n",
      "\t Val. Loss: 0.0017084431\n",
      "Epoch: 185 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017535962\n",
      "\t Val. Loss: 0.0017086048\n",
      "Epoch: 186 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017391290\n",
      "\t Val. Loss: 0.0017081709\n",
      "Epoch: 187 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017639191\n",
      "\t Val. Loss: 0.0017090577\n",
      "Epoch: 188 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017629358\n",
      "\t Val. Loss: 0.0017099126\n",
      "Epoch: 189 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017688005\n",
      "\t Val. Loss: 0.0017097347\n",
      "Epoch: 190 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017851741\n",
      "\t Val. Loss: 0.0017092825\n",
      "Epoch: 191 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017909682\n",
      "\t Val. Loss: 0.0017087283\n",
      "Epoch: 192 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017530982\n",
      "\t Val. Loss: 0.0017083515\n",
      "Epoch: 193 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017799181\n",
      "\t Val. Loss: 0.0017085806\n",
      "Epoch: 194 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017564783\n",
      "\t Val. Loss: 0.0017078500\n",
      "Epoch: 195 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017682128\n",
      "\t Val. Loss: 0.0017077068\n",
      "Epoch: 196 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017290928\n",
      "\t Val. Loss: 0.0017074781\n",
      "Epoch: 197 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017596363\n",
      "\t Val. Loss: 0.0017065144\n",
      "Epoch: 198 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017933059\n",
      "\t Val. Loss: 0.0017057970\n",
      "Epoch: 199 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017732593\n",
      "\t Val. Loss: 0.0017056976\n",
      "Epoch: 200 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017301282\n",
      "\t Val. Loss: 0.0017052694\n",
      "Epoch: 201 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017625843\n",
      "\t Val. Loss: 0.0017048983\n",
      "Epoch: 202 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017459333\n",
      "\t Val. Loss: 0.0017055288\n",
      "Epoch: 203 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017631952\n",
      "\t Val. Loss: 0.0017063577\n",
      "Epoch: 204 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017305858\n",
      "\t Val. Loss: 0.0017064539\n",
      "Epoch: 205 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017634557\n",
      "\t Val. Loss: 0.0017070357\n",
      "Epoch: 206 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017621623\n",
      "\t Val. Loss: 0.0017064995\n",
      "Epoch: 207 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017504797\n",
      "\t Val. Loss: 0.0017057292\n",
      "Epoch: 208 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017892057\n",
      "\t Val. Loss: 0.0017059598\n",
      "Epoch: 209 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017625175\n",
      "\t Val. Loss: 0.0017058433\n",
      "Epoch: 210 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017541614\n",
      "\t Val. Loss: 0.0017058948\n",
      "Epoch: 211 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017607301\n",
      "\t Val. Loss: 0.0017060645\n",
      "Epoch: 212 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017550056\n",
      "\t Val. Loss: 0.0017057967\n",
      "Epoch: 213 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017722775\n",
      "\t Val. Loss: 0.0017048995\n",
      "Epoch: 214 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017535025\n",
      "\t Val. Loss: 0.0017055423\n",
      "Epoch: 215 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017598370\n",
      "\t Val. Loss: 0.0017055530\n",
      "Epoch: 216 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017394666\n",
      "\t Val. Loss: 0.0017055284\n",
      "Epoch: 217 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017222796\n",
      "\t Val. Loss: 0.0017057876\n",
      "Epoch: 218 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017795672\n",
      "\t Val. Loss: 0.0017062869\n",
      "Epoch: 219 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017774577\n",
      "\t Val. Loss: 0.0017061817\n",
      "Epoch: 220 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017497364\n",
      "\t Val. Loss: 0.0017062008\n",
      "Epoch: 221 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017723744\n",
      "\t Val. Loss: 0.0017067072\n",
      "Epoch: 222 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017567196\n",
      "\t Val. Loss: 0.0017074549\n",
      "Epoch: 223 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017487536\n",
      "\t Val. Loss: 0.0017066129\n",
      "Epoch: 224 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017694582\n",
      "\t Val. Loss: 0.0017062409\n",
      "Epoch: 225 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017449166\n",
      "\t Val. Loss: 0.0017059036\n",
      "Epoch: 226 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017656244\n",
      "\t Val. Loss: 0.0017060236\n",
      "Epoch: 227 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017612889\n",
      "\t Val. Loss: 0.0017063089\n",
      "Epoch: 228 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017767902\n",
      "\t Val. Loss: 0.0017065223\n",
      "Epoch: 229 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017451394\n",
      "\t Val. Loss: 0.0017067310\n",
      "Epoch: 230 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017696696\n",
      "\t Val. Loss: 0.0017076220\n",
      "Epoch: 231 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017551783\n",
      "\t Val. Loss: 0.0017075656\n",
      "Epoch: 232 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017762451\n",
      "\t Val. Loss: 0.0017075104\n",
      "Epoch: 233 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017906327\n",
      "\t Val. Loss: 0.0017065409\n",
      "Epoch: 234 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017673817\n",
      "\t Val. Loss: 0.0017068021\n",
      "Epoch: 235 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017670756\n",
      "\t Val. Loss: 0.0017066857\n",
      "Epoch: 236 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017723576\n",
      "\t Val. Loss: 0.0017061885\n",
      "Epoch: 237 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017390332\n",
      "\t Val. Loss: 0.0017060351\n",
      "Epoch: 238 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017719167\n",
      "\t Val. Loss: 0.0017056170\n",
      "Epoch: 239 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017389993\n",
      "\t Val. Loss: 0.0017048321\n",
      "Epoch: 240 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017454696\n",
      "\t Val. Loss: 0.0017050968\n",
      "Epoch: 241 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017644757\n",
      "\t Val. Loss: 0.0017062129\n",
      "Epoch: 242 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017473961\n",
      "\t Val. Loss: 0.0017053094\n",
      "Epoch: 243 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017580616\n",
      "\t Val. Loss: 0.0017058230\n",
      "Epoch: 244 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017406989\n",
      "\t Val. Loss: 0.0017051233\n",
      "Epoch: 245 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017576969\n",
      "\t Val. Loss: 0.0017054587\n",
      "Epoch: 246 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017998105\n",
      "\t Val. Loss: 0.0017056677\n",
      "Epoch: 247 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017884929\n",
      "\t Val. Loss: 0.0017062139\n",
      "Epoch: 248 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017586431\n",
      "\t Val. Loss: 0.0017060542\n",
      "Epoch: 249 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017878132\n",
      "\t Val. Loss: 0.0017060213\n",
      "Epoch: 250 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017549405\n",
      "\t Val. Loss: 0.0017062877\n",
      "Epoch: 251 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017460307\n",
      "\t Val. Loss: 0.0017057474\n",
      "Epoch: 252 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017735600\n",
      "\t Val. Loss: 0.0017065038\n",
      "Epoch: 253 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017661121\n",
      "\t Val. Loss: 0.0017061864\n",
      "Epoch: 254 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017909154\n",
      "\t Val. Loss: 0.0017067125\n",
      "Epoch: 255 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017783870\n",
      "\t Val. Loss: 0.0017072902\n",
      "Epoch: 256 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017749081\n",
      "\t Val. Loss: 0.0017069680\n",
      "Epoch: 257 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017700022\n",
      "\t Val. Loss: 0.0017075303\n",
      "Epoch: 258 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017746624\n",
      "\t Val. Loss: 0.0017075686\n",
      "Epoch: 259 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017366159\n",
      "\t Val. Loss: 0.0017072222\n",
      "Epoch: 260 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017464268\n",
      "\t Val. Loss: 0.0017076374\n",
      "Epoch: 261 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017494906\n",
      "\t Val. Loss: 0.0017071632\n",
      "Epoch: 262 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017517033\n",
      "\t Val. Loss: 0.0017073337\n",
      "Epoch: 263 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017387874\n",
      "\t Val. Loss: 0.0017074264\n",
      "Epoch: 264 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017423736\n",
      "\t Val. Loss: 0.0017066909\n",
      "Epoch: 265 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017656318\n",
      "\t Val. Loss: 0.0017049892\n",
      "Epoch: 266 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017515155\n",
      "\t Val. Loss: 0.0017048818\n",
      "Epoch: 267 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017404155\n",
      "\t Val. Loss: 0.0017054112\n",
      "Epoch: 268 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017663691\n",
      "\t Val. Loss: 0.0017049497\n",
      "Epoch: 269 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017648740\n",
      "\t Val. Loss: 0.0017050916\n",
      "Epoch: 270 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017274988\n",
      "\t Val. Loss: 0.0017045593\n",
      "Epoch: 271 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017487571\n",
      "\t Val. Loss: 0.0017045803\n",
      "Epoch: 272 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017527575\n",
      "\t Val. Loss: 0.0017051086\n",
      "Epoch: 273 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017460983\n",
      "\t Val. Loss: 0.0017044979\n",
      "Epoch: 274 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017250216\n",
      "\t Val. Loss: 0.0017041952\n",
      "Epoch: 275 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017590320\n",
      "\t Val. Loss: 0.0017040696\n",
      "Epoch: 276 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017963077\n",
      "\t Val. Loss: 0.0017050780\n",
      "Epoch: 277 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017480549\n",
      "\t Val. Loss: 0.0017050015\n",
      "Epoch: 278 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017352908\n",
      "\t Val. Loss: 0.0017056064\n",
      "Epoch: 279 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017597637\n",
      "\t Val. Loss: 0.0017055248\n",
      "Epoch: 280 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017482655\n",
      "\t Val. Loss: 0.0017051597\n",
      "Epoch: 281 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017451369\n",
      "\t Val. Loss: 0.0017050204\n",
      "Epoch: 282 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017543618\n",
      "\t Val. Loss: 0.0017052817\n",
      "Epoch: 283 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017545754\n",
      "\t Val. Loss: 0.0017058973\n",
      "Epoch: 284 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017478008\n",
      "\t Val. Loss: 0.0017056546\n",
      "Epoch: 285 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017259284\n",
      "\t Val. Loss: 0.0017050082\n",
      "Epoch: 286 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017885646\n",
      "\t Val. Loss: 0.0017054039\n",
      "Epoch: 287 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017493318\n",
      "\t Val. Loss: 0.0017047261\n",
      "Epoch: 288 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017522263\n",
      "\t Val. Loss: 0.0017053014\n",
      "Epoch: 289 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017444294\n",
      "\t Val. Loss: 0.0017056636\n",
      "Epoch: 290 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017420133\n",
      "\t Val. Loss: 0.0017053337\n",
      "Epoch: 291 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017557198\n",
      "\t Val. Loss: 0.0017052733\n",
      "Epoch: 292 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017575943\n",
      "\t Val. Loss: 0.0017048756\n",
      "Epoch: 293 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017839027\n",
      "\t Val. Loss: 0.0017052230\n",
      "Epoch: 294 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017663764\n",
      "\t Val. Loss: 0.0017056172\n",
      "Epoch: 295 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017434765\n",
      "\t Val. Loss: 0.0017054119\n",
      "Epoch: 296 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017511874\n",
      "\t Val. Loss: 0.0017054660\n",
      "Epoch: 297 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017494573\n",
      "\t Val. Loss: 0.0017057788\n",
      "Epoch: 298 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017722056\n",
      "\t Val. Loss: 0.0017058911\n",
      "Epoch: 299 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017572814\n",
      "\t Val. Loss: 0.0017059927\n",
      "Epoch: 300 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017447730\n",
      "\t Val. Loss: 0.0017060945\n",
      "Epoch: 301 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017321206\n",
      "\t Val. Loss: 0.0017061334\n",
      "Epoch: 302 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017656516\n",
      "\t Val. Loss: 0.0017065741\n",
      "Epoch: 303 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017544355\n",
      "\t Val. Loss: 0.0017069008\n",
      "Epoch: 304 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017380873\n",
      "\t Val. Loss: 0.0017074703\n",
      "Epoch: 305 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017460257\n",
      "\t Val. Loss: 0.0017072368\n",
      "Epoch: 306 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017388265\n",
      "\t Val. Loss: 0.0017067246\n",
      "Epoch: 307 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017391378\n",
      "\t Val. Loss: 0.0017069195\n",
      "Epoch: 308 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017551445\n",
      "\t Val. Loss: 0.0017061713\n",
      "Epoch: 309 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017573485\n",
      "\t Val. Loss: 0.0017062963\n",
      "Epoch: 310 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017565339\n",
      "\t Val. Loss: 0.0017060910\n",
      "Epoch: 311 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017665176\n",
      "\t Val. Loss: 0.0017058932\n",
      "Epoch: 312 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017441656\n",
      "\t Val. Loss: 0.0017055134\n",
      "Epoch: 313 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017351024\n",
      "\t Val. Loss: 0.0017054273\n",
      "Epoch: 314 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017733366\n",
      "\t Val. Loss: 0.0017052885\n",
      "Epoch: 315 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017456379\n",
      "\t Val. Loss: 0.0017056114\n",
      "Epoch: 316 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017529679\n",
      "\t Val. Loss: 0.0017062181\n",
      "Epoch: 317 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017608957\n",
      "\t Val. Loss: 0.0017060664\n",
      "Epoch: 318 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017414357\n",
      "\t Val. Loss: 0.0017059877\n",
      "Epoch: 319 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017355289\n",
      "\t Val. Loss: 0.0017057361\n",
      "Epoch: 320 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017473346\n",
      "\t Val. Loss: 0.0017054808\n",
      "Epoch: 321 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017662594\n",
      "\t Val. Loss: 0.0017051011\n",
      "Epoch: 322 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017320888\n",
      "\t Val. Loss: 0.0017057212\n",
      "Epoch: 323 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017444766\n",
      "\t Val. Loss: 0.0017052564\n",
      "Epoch: 324 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017345855\n",
      "\t Val. Loss: 0.0017044675\n",
      "Epoch: 325 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017373206\n",
      "\t Val. Loss: 0.0017037083\n",
      "Epoch: 326 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017443224\n",
      "\t Val. Loss: 0.0017039883\n",
      "Epoch: 327 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017349169\n",
      "\t Val. Loss: 0.0017040241\n",
      "Epoch: 328 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018039543\n",
      "\t Val. Loss: 0.0017042216\n",
      "Epoch: 329 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018000273\n",
      "\t Val. Loss: 0.0017048290\n",
      "Epoch: 330 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017558378\n",
      "\t Val. Loss: 0.0017050999\n",
      "Epoch: 331 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017462745\n",
      "\t Val. Loss: 0.0017050733\n",
      "Epoch: 332 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017445017\n",
      "\t Val. Loss: 0.0017048206\n",
      "Epoch: 333 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017480666\n",
      "\t Val. Loss: 0.0017048065\n",
      "Epoch: 334 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017631332\n",
      "\t Val. Loss: 0.0017042559\n",
      "Epoch: 335 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017144707\n",
      "\t Val. Loss: 0.0017042240\n",
      "Epoch: 336 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017467885\n",
      "\t Val. Loss: 0.0017033331\n",
      "Epoch: 337 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017476176\n",
      "\t Val. Loss: 0.0017032913\n",
      "Epoch: 338 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017462600\n",
      "\t Val. Loss: 0.0017035254\n",
      "Epoch: 339 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017384087\n",
      "\t Val. Loss: 0.0017036029\n",
      "Epoch: 340 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017565166\n",
      "\t Val. Loss: 0.0017041883\n",
      "Epoch: 341 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017641335\n",
      "\t Val. Loss: 0.0017052567\n",
      "Epoch: 342 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017438637\n",
      "\t Val. Loss: 0.0017050014\n",
      "Epoch: 343 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017219919\n",
      "\t Val. Loss: 0.0017047555\n",
      "Epoch: 344 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017571198\n",
      "\t Val. Loss: 0.0017041227\n",
      "Epoch: 345 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017489108\n",
      "\t Val. Loss: 0.0017043090\n",
      "Epoch: 346 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017662042\n",
      "\t Val. Loss: 0.0017050393\n",
      "Epoch: 347 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017526600\n",
      "\t Val. Loss: 0.0017054114\n",
      "Epoch: 348 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017742654\n",
      "\t Val. Loss: 0.0017059285\n",
      "Epoch: 349 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017457955\n",
      "\t Val. Loss: 0.0017057000\n",
      "Epoch: 350 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017398630\n",
      "\t Val. Loss: 0.0017048487\n",
      "Epoch: 351 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017546443\n",
      "\t Val. Loss: 0.0017050684\n",
      "Epoch: 352 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017420818\n",
      "\t Val. Loss: 0.0017051893\n",
      "Epoch: 353 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017232614\n",
      "\t Val. Loss: 0.0017042346\n",
      "Epoch: 354 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017342130\n",
      "\t Val. Loss: 0.0017036534\n",
      "Epoch: 355 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017479713\n",
      "\t Val. Loss: 0.0017040634\n",
      "Epoch: 356 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017507718\n",
      "\t Val. Loss: 0.0017044562\n",
      "Epoch: 357 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017619462\n",
      "\t Val. Loss: 0.0017043426\n",
      "Epoch: 358 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017414173\n",
      "\t Val. Loss: 0.0017045018\n",
      "Epoch: 359 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017380358\n",
      "\t Val. Loss: 0.0017038912\n",
      "Epoch: 360 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017512017\n",
      "\t Val. Loss: 0.0017035525\n",
      "Epoch: 361 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017610384\n",
      "\t Val. Loss: 0.0017030775\n",
      "Epoch: 362 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017391770\n",
      "\t Val. Loss: 0.0017030237\n",
      "Epoch: 363 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017707159\n",
      "\t Val. Loss: 0.0017034847\n",
      "Epoch: 364 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017332740\n",
      "\t Val. Loss: 0.0017037670\n",
      "Epoch: 365 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017427108\n",
      "\t Val. Loss: 0.0017034290\n",
      "Epoch: 366 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017498130\n",
      "\t Val. Loss: 0.0017041603\n",
      "Epoch: 367 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017437714\n",
      "\t Val. Loss: 0.0017031332\n",
      "Epoch: 368 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017427385\n",
      "\t Val. Loss: 0.0017036150\n",
      "Epoch: 369 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017299367\n",
      "\t Val. Loss: 0.0017047067\n",
      "Epoch: 370 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017378309\n",
      "\t Val. Loss: 0.0017037665\n",
      "Epoch: 371 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017507130\n",
      "\t Val. Loss: 0.0017034006\n",
      "Epoch: 372 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017229636\n",
      "\t Val. Loss: 0.0017026278\n",
      "Epoch: 373 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017401379\n",
      "\t Val. Loss: 0.0017022663\n",
      "Epoch: 374 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017772508\n",
      "\t Val. Loss: 0.0017021199\n",
      "Epoch: 375 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017388313\n",
      "\t Val. Loss: 0.0017032688\n",
      "Epoch: 376 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017472903\n",
      "\t Val. Loss: 0.0017036283\n",
      "Epoch: 377 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017528926\n",
      "\t Val. Loss: 0.0017036064\n",
      "Epoch: 378 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017552862\n",
      "\t Val. Loss: 0.0017038093\n",
      "Epoch: 379 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017610463\n",
      "\t Val. Loss: 0.0017041357\n",
      "Epoch: 380 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017616801\n",
      "\t Val. Loss: 0.0017044787\n",
      "Epoch: 381 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017333408\n",
      "\t Val. Loss: 0.0017039248\n",
      "Epoch: 382 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017361822\n",
      "\t Val. Loss: 0.0017042612\n",
      "Epoch: 383 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017505738\n",
      "\t Val. Loss: 0.0017040611\n",
      "Epoch: 384 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017659119\n",
      "\t Val. Loss: 0.0017042524\n",
      "Epoch: 385 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017454006\n",
      "\t Val. Loss: 0.0017037108\n",
      "Epoch: 386 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017304385\n",
      "\t Val. Loss: 0.0017041273\n",
      "Epoch: 387 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017327704\n",
      "\t Val. Loss: 0.0017032085\n",
      "Epoch: 388 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017428658\n",
      "\t Val. Loss: 0.0017032410\n",
      "Epoch: 389 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017437711\n",
      "\t Val. Loss: 0.0017028875\n",
      "Epoch: 390 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017262616\n",
      "\t Val. Loss: 0.0017029785\n",
      "Epoch: 391 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017488863\n",
      "\t Val. Loss: 0.0017035857\n",
      "Epoch: 392 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017430145\n",
      "\t Val. Loss: 0.0017032414\n",
      "Epoch: 393 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017446054\n",
      "\t Val. Loss: 0.0017023069\n",
      "Epoch: 394 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017466637\n",
      "\t Val. Loss: 0.0017021260\n",
      "Epoch: 395 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017429210\n",
      "\t Val. Loss: 0.0017026474\n",
      "Epoch: 396 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017398566\n",
      "\t Val. Loss: 0.0017034071\n",
      "Epoch: 397 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017637611\n",
      "\t Val. Loss: 0.0017029759\n",
      "Epoch: 398 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017405580\n",
      "\t Val. Loss: 0.0017040667\n",
      "Epoch: 399 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017369118\n",
      "\t Val. Loss: 0.0017043476\n",
      "Epoch: 400 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017299455\n",
      "\t Val. Loss: 0.0017037430\n",
      "Epoch: 401 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017464966\n",
      "\t Val. Loss: 0.0017040750\n",
      "Epoch: 402 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017446341\n",
      "\t Val. Loss: 0.0017051474\n",
      "Epoch: 403 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017405844\n",
      "\t Val. Loss: 0.0017047089\n",
      "Epoch: 404 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017267720\n",
      "\t Val. Loss: 0.0017046622\n",
      "Epoch: 405 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017261020\n",
      "\t Val. Loss: 0.0017037044\n",
      "Epoch: 406 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017367457\n",
      "\t Val. Loss: 0.0017027926\n",
      "Epoch: 407 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017469915\n",
      "\t Val. Loss: 0.0017023258\n",
      "Epoch: 408 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017245444\n",
      "\t Val. Loss: 0.0017025537\n",
      "Epoch: 409 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017340416\n",
      "\t Val. Loss: 0.0017020670\n",
      "Epoch: 410 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017390338\n",
      "\t Val. Loss: 0.0017015885\n",
      "Epoch: 411 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017142040\n",
      "\t Val. Loss: 0.0017016671\n",
      "Epoch: 412 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017373702\n",
      "\t Val. Loss: 0.0017009562\n",
      "Epoch: 413 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017328295\n",
      "\t Val. Loss: 0.0017005173\n",
      "Epoch: 414 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017491461\n",
      "\t Val. Loss: 0.0017002995\n",
      "Epoch: 415 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017358519\n",
      "\t Val. Loss: 0.0017011568\n",
      "Epoch: 416 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017218898\n",
      "\t Val. Loss: 0.0017014155\n",
      "Epoch: 417 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017276843\n",
      "\t Val. Loss: 0.0017013621\n",
      "Epoch: 418 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017283046\n",
      "\t Val. Loss: 0.0017016028\n",
      "Epoch: 419 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017360610\n",
      "\t Val. Loss: 0.0017011964\n",
      "Epoch: 420 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017394544\n",
      "\t Val. Loss: 0.0017008132\n",
      "Epoch: 421 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017278647\n",
      "\t Val. Loss: 0.0017003749\n",
      "Epoch: 422 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017518663\n",
      "\t Val. Loss: 0.0017008327\n",
      "Epoch: 423 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017328602\n",
      "\t Val. Loss: 0.0017000074\n",
      "Epoch: 424 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017488779\n",
      "\t Val. Loss: 0.0017002976\n",
      "Epoch: 425 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017426236\n",
      "\t Val. Loss: 0.0017006621\n",
      "Epoch: 426 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017588017\n",
      "\t Val. Loss: 0.0017007602\n",
      "Epoch: 427 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017561362\n",
      "\t Val. Loss: 0.0017013219\n",
      "Epoch: 428 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017278668\n",
      "\t Val. Loss: 0.0017013284\n",
      "Epoch: 429 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017555650\n",
      "\t Val. Loss: 0.0017014028\n",
      "Epoch: 430 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017214191\n",
      "\t Val. Loss: 0.0017008341\n",
      "Epoch: 431 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017314727\n",
      "\t Val. Loss: 0.0017010467\n",
      "Epoch: 432 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017219892\n",
      "\t Val. Loss: 0.0017009141\n",
      "Epoch: 433 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017375254\n",
      "\t Val. Loss: 0.0017014736\n",
      "Epoch: 434 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017505043\n",
      "\t Val. Loss: 0.0017023704\n",
      "Epoch: 435 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017399307\n",
      "\t Val. Loss: 0.0017024966\n",
      "Epoch: 436 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017367158\n",
      "\t Val. Loss: 0.0017025854\n",
      "Epoch: 437 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017622065\n",
      "\t Val. Loss: 0.0017027904\n",
      "Epoch: 438 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017502508\n",
      "\t Val. Loss: 0.0017032436\n",
      "Epoch: 439 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017462499\n",
      "\t Val. Loss: 0.0017031966\n",
      "Epoch: 440 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017503552\n",
      "\t Val. Loss: 0.0017030647\n",
      "Epoch: 441 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017329117\n",
      "\t Val. Loss: 0.0017027443\n",
      "Epoch: 442 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017364659\n",
      "\t Val. Loss: 0.0017022879\n",
      "Epoch: 443 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017446470\n",
      "\t Val. Loss: 0.0017024986\n",
      "Epoch: 444 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017337760\n",
      "\t Val. Loss: 0.0017026752\n",
      "Epoch: 445 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017503394\n",
      "\t Val. Loss: 0.0017016627\n",
      "Epoch: 446 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017408133\n",
      "\t Val. Loss: 0.0017016329\n",
      "Epoch: 447 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017415705\n",
      "\t Val. Loss: 0.0017017739\n",
      "Epoch: 448 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017298305\n",
      "\t Val. Loss: 0.0017018186\n",
      "Epoch: 449 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017286814\n",
      "\t Val. Loss: 0.0017022181\n",
      "Epoch: 450 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017407349\n",
      "\t Val. Loss: 0.0017016350\n",
      "Epoch: 451 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017131717\n",
      "\t Val. Loss: 0.0017008673\n",
      "Epoch: 452 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017222434\n",
      "\t Val. Loss: 0.0017001543\n",
      "Epoch: 453 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017467691\n",
      "\t Val. Loss: 0.0017004339\n",
      "Epoch: 454 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017427728\n",
      "\t Val. Loss: 0.0017000665\n",
      "Epoch: 455 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017534030\n",
      "\t Val. Loss: 0.0017002771\n",
      "Epoch: 456 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017256485\n",
      "\t Val. Loss: 0.0017004718\n",
      "Epoch: 457 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017133690\n",
      "\t Val. Loss: 0.0017007134\n",
      "Epoch: 458 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017361514\n",
      "\t Val. Loss: 0.0017015471\n",
      "Epoch: 459 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017488411\n",
      "\t Val. Loss: 0.0017012593\n",
      "Epoch: 460 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017617261\n",
      "\t Val. Loss: 0.0017010004\n",
      "Epoch: 461 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017463885\n",
      "\t Val. Loss: 0.0017010134\n",
      "Epoch: 462 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017452725\n",
      "\t Val. Loss: 0.0017010489\n",
      "Epoch: 463 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017506371\n",
      "\t Val. Loss: 0.0017007067\n",
      "Epoch: 464 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017315536\n",
      "\t Val. Loss: 0.0017001913\n",
      "Epoch: 465 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017616522\n",
      "\t Val. Loss: 0.0017007876\n",
      "Epoch: 466 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017642929\n",
      "\t Val. Loss: 0.0017009316\n",
      "Epoch: 467 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017283559\n",
      "\t Val. Loss: 0.0017010292\n",
      "Epoch: 468 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017563755\n",
      "\t Val. Loss: 0.0017010774\n",
      "Epoch: 469 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017412806\n",
      "\t Val. Loss: 0.0017014277\n",
      "Epoch: 470 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017427012\n",
      "\t Val. Loss: 0.0017006762\n",
      "Epoch: 471 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017275179\n",
      "\t Val. Loss: 0.0017009147\n",
      "Epoch: 472 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017312941\n",
      "\t Val. Loss: 0.0017000009\n",
      "Epoch: 473 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017241106\n",
      "\t Val. Loss: 0.0017001100\n",
      "Epoch: 474 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017434651\n",
      "\t Val. Loss: 0.0017002567\n",
      "Epoch: 475 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017572308\n",
      "\t Val. Loss: 0.0017004861\n",
      "Epoch: 476 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017262051\n",
      "\t Val. Loss: 0.0017003816\n",
      "Epoch: 477 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017474971\n",
      "\t Val. Loss: 0.0017018256\n",
      "Epoch: 478 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017494130\n",
      "\t Val. Loss: 0.0017019760\n",
      "Epoch: 479 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017474971\n",
      "\t Val. Loss: 0.0017018291\n",
      "Epoch: 480 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017511957\n",
      "\t Val. Loss: 0.0017017229\n",
      "Epoch: 481 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017256461\n",
      "\t Val. Loss: 0.0017022214\n",
      "Epoch: 482 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017518522\n",
      "\t Val. Loss: 0.0017023920\n",
      "Epoch: 483 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017562587\n",
      "\t Val. Loss: 0.0017022738\n",
      "Epoch: 484 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017434727\n",
      "\t Val. Loss: 0.0017016110\n",
      "Epoch: 485 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017316797\n",
      "\t Val. Loss: 0.0017011096\n",
      "Epoch: 486 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017368330\n",
      "\t Val. Loss: 0.0017013052\n",
      "Epoch: 487 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017284933\n",
      "\t Val. Loss: 0.0017010688\n",
      "Epoch: 488 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017346495\n",
      "\t Val. Loss: 0.0017004341\n",
      "Epoch: 489 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017101164\n",
      "\t Val. Loss: 0.0017001162\n",
      "Epoch: 490 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017360712\n",
      "\t Val. Loss: 0.0016993412\n",
      "Epoch: 491 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017129776\n",
      "\t Val. Loss: 0.0016992806\n",
      "Epoch: 492 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017514081\n",
      "\t Val. Loss: 0.0017002085\n",
      "Epoch: 493 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017428794\n",
      "\t Val. Loss: 0.0017003287\n",
      "Epoch: 494 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017339364\n",
      "\t Val. Loss: 0.0017009328\n",
      "Epoch: 495 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017295044\n",
      "\t Val. Loss: 0.0017011412\n",
      "Epoch: 496 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017295127\n",
      "\t Val. Loss: 0.0017013512\n",
      "Epoch: 497 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017460560\n",
      "\t Val. Loss: 0.0017017685\n",
      "Epoch: 498 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017461128\n",
      "\t Val. Loss: 0.0017015389\n",
      "Epoch: 499 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017461268\n",
      "\t Val. Loss: 0.0017011821\n",
      "Epoch: 500 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017336377\n",
      "\t Val. Loss: 0.0017011725\n",
      "Epoch: 501 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017174625\n",
      "\t Val. Loss: 0.0017002939\n",
      "Epoch: 502 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017241793\n",
      "\t Val. Loss: 0.0017003695\n",
      "Epoch: 503 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017435793\n",
      "\t Val. Loss: 0.0017002796\n",
      "Epoch: 504 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017494726\n",
      "\t Val. Loss: 0.0017006378\n",
      "Epoch: 505 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017162294\n",
      "\t Val. Loss: 0.0017008330\n",
      "Epoch: 506 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017250651\n",
      "\t Val. Loss: 0.0017004211\n",
      "Epoch: 507 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017439057\n",
      "\t Val. Loss: 0.0017008760\n",
      "Epoch: 508 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017282141\n",
      "\t Val. Loss: 0.0017010871\n",
      "Epoch: 509 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017217551\n",
      "\t Val. Loss: 0.0017013628\n",
      "Epoch: 510 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017421979\n",
      "\t Val. Loss: 0.0017019710\n",
      "Epoch: 511 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017374919\n",
      "\t Val. Loss: 0.0017018641\n",
      "Epoch: 512 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017411625\n",
      "\t Val. Loss: 0.0017014684\n",
      "Epoch: 513 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017298613\n",
      "\t Val. Loss: 0.0017006167\n",
      "Epoch: 514 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017588739\n",
      "\t Val. Loss: 0.0017005883\n",
      "Epoch: 515 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017148526\n",
      "\t Val. Loss: 0.0017003767\n",
      "Epoch: 516 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017304092\n",
      "\t Val. Loss: 0.0017003609\n",
      "Epoch: 517 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017464128\n",
      "\t Val. Loss: 0.0017005619\n",
      "Epoch: 518 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017602604\n",
      "\t Val. Loss: 0.0017001606\n",
      "Epoch: 519 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017159962\n",
      "\t Val. Loss: 0.0016999704\n",
      "Epoch: 520 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017425870\n",
      "\t Val. Loss: 0.0016997554\n",
      "Epoch: 521 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017346477\n",
      "\t Val. Loss: 0.0016997567\n",
      "Epoch: 522 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017273918\n",
      "\t Val. Loss: 0.0016994405\n",
      "Epoch: 523 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017293408\n",
      "\t Val. Loss: 0.0016987416\n",
      "Epoch: 524 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017295110\n",
      "\t Val. Loss: 0.0016986359\n",
      "Epoch: 525 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017358932\n",
      "\t Val. Loss: 0.0016980187\n",
      "Epoch: 526 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017471619\n",
      "\t Val. Loss: 0.0016978963\n",
      "Epoch: 527 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017129943\n",
      "\t Val. Loss: 0.0016979780\n",
      "Epoch: 528 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017240843\n",
      "\t Val. Loss: 0.0016978818\n",
      "Epoch: 529 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017065255\n",
      "\t Val. Loss: 0.0016981816\n",
      "Epoch: 530 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017536813\n",
      "\t Val. Loss: 0.0016986080\n",
      "Epoch: 531 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017451203\n",
      "\t Val. Loss: 0.0016985942\n",
      "Epoch: 532 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017393208\n",
      "\t Val. Loss: 0.0016994545\n",
      "Epoch: 533 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017504831\n",
      "\t Val. Loss: 0.0016994078\n",
      "Epoch: 534 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017300957\n",
      "\t Val. Loss: 0.0016994913\n",
      "Epoch: 535 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017126531\n",
      "\t Val. Loss: 0.0016993509\n",
      "Epoch: 536 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017293187\n",
      "\t Val. Loss: 0.0016989995\n",
      "Epoch: 537 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017149881\n",
      "\t Val. Loss: 0.0016994497\n",
      "Epoch: 538 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017326060\n",
      "\t Val. Loss: 0.0016990167\n",
      "Epoch: 539 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017405347\n",
      "\t Val. Loss: 0.0016994935\n",
      "Epoch: 540 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017294264\n",
      "\t Val. Loss: 0.0017002686\n",
      "Epoch: 541 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017364926\n",
      "\t Val. Loss: 0.0017007779\n",
      "Epoch: 542 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017375072\n",
      "\t Val. Loss: 0.0017005363\n",
      "Epoch: 543 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017465450\n",
      "\t Val. Loss: 0.0017005837\n",
      "Epoch: 544 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017299290\n",
      "\t Val. Loss: 0.0017005269\n",
      "Epoch: 545 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017086869\n",
      "\t Val. Loss: 0.0017008706\n",
      "Epoch: 546 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017611500\n",
      "\t Val. Loss: 0.0017005480\n",
      "Epoch: 547 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017307170\n",
      "\t Val. Loss: 0.0017007062\n",
      "Epoch: 548 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017145738\n",
      "\t Val. Loss: 0.0017016428\n",
      "Epoch: 549 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017493145\n",
      "\t Val. Loss: 0.0017010354\n",
      "Epoch: 550 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017352195\n",
      "\t Val. Loss: 0.0017014964\n",
      "Epoch: 551 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017307022\n",
      "\t Val. Loss: 0.0017013291\n",
      "Epoch: 552 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017407696\n",
      "\t Val. Loss: 0.0017014457\n",
      "Epoch: 553 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017259136\n",
      "\t Val. Loss: 0.0017009184\n",
      "Epoch: 554 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017120959\n",
      "\t Val. Loss: 0.0017008824\n",
      "Epoch: 555 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017054288\n",
      "\t Val. Loss: 0.0017011167\n",
      "Epoch: 556 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017085297\n",
      "\t Val. Loss: 0.0017017106\n",
      "Epoch: 557 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017474277\n",
      "\t Val. Loss: 0.0017020876\n",
      "Epoch: 558 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017133495\n",
      "\t Val. Loss: 0.0017017754\n",
      "Epoch: 559 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017675196\n",
      "\t Val. Loss: 0.0017021009\n",
      "Epoch: 560 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017143696\n",
      "\t Val. Loss: 0.0017017130\n",
      "Epoch: 561 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017290692\n",
      "\t Val. Loss: 0.0017011731\n",
      "Epoch: 562 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017229816\n",
      "\t Val. Loss: 0.0017002062\n",
      "Epoch: 563 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017175763\n",
      "\t Val. Loss: 0.0016999778\n",
      "Epoch: 564 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017373340\n",
      "\t Val. Loss: 0.0016996582\n",
      "Epoch: 565 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017247130\n",
      "\t Val. Loss: 0.0017001594\n",
      "Epoch: 566 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017487224\n",
      "\t Val. Loss: 0.0016994609\n",
      "Epoch: 567 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017351503\n",
      "\t Val. Loss: 0.0016996789\n",
      "Epoch: 568 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017230765\n",
      "\t Val. Loss: 0.0016993354\n",
      "Epoch: 569 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017423243\n",
      "\t Val. Loss: 0.0016996742\n",
      "Epoch: 570 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017429947\n",
      "\t Val. Loss: 0.0016999046\n",
      "Epoch: 571 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017343052\n",
      "\t Val. Loss: 0.0016997394\n",
      "Epoch: 572 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017247223\n",
      "\t Val. Loss: 0.0017000213\n",
      "Epoch: 573 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017027429\n",
      "\t Val. Loss: 0.0017008523\n",
      "Epoch: 574 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017440570\n",
      "\t Val. Loss: 0.0017011358\n",
      "Epoch: 575 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017338832\n",
      "\t Val. Loss: 0.0017016107\n",
      "Epoch: 576 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017239365\n",
      "\t Val. Loss: 0.0017014671\n",
      "Epoch: 577 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017387278\n",
      "\t Val. Loss: 0.0017004915\n",
      "Epoch: 578 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017098741\n",
      "\t Val. Loss: 0.0016997088\n",
      "Epoch: 579 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017127314\n",
      "\t Val. Loss: 0.0016991548\n",
      "Epoch: 580 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017127124\n",
      "\t Val. Loss: 0.0016992261\n",
      "Epoch: 581 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017499139\n",
      "\t Val. Loss: 0.0016999862\n",
      "Epoch: 582 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017307648\n",
      "\t Val. Loss: 0.0017007575\n",
      "Epoch: 583 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017278769\n",
      "\t Val. Loss: 0.0016999603\n",
      "Epoch: 584 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017337642\n",
      "\t Val. Loss: 0.0016995876\n",
      "Epoch: 585 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017189459\n",
      "\t Val. Loss: 0.0016993601\n",
      "Epoch: 586 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017398436\n",
      "\t Val. Loss: 0.0016992134\n",
      "Epoch: 587 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017383122\n",
      "\t Val. Loss: 0.0016991459\n",
      "Epoch: 588 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017275486\n",
      "\t Val. Loss: 0.0016994274\n",
      "Epoch: 589 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017804682\n",
      "\t Val. Loss: 0.0016989788\n",
      "Epoch: 590 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017221403\n",
      "\t Val. Loss: 0.0016985383\n",
      "Epoch: 591 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017494979\n",
      "\t Val. Loss: 0.0016985872\n",
      "Epoch: 592 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017452049\n",
      "\t Val. Loss: 0.0016984546\n",
      "Epoch: 593 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017314198\n",
      "\t Val. Loss: 0.0016981665\n",
      "Epoch: 594 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017424096\n",
      "\t Val. Loss: 0.0016990264\n",
      "Epoch: 595 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017201711\n",
      "\t Val. Loss: 0.0016978526\n",
      "Epoch: 596 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017221639\n",
      "\t Val. Loss: 0.0016982396\n",
      "Epoch: 597 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017571071\n",
      "\t Val. Loss: 0.0016989887\n",
      "Epoch: 598 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017479346\n",
      "\t Val. Loss: 0.0016992721\n",
      "Epoch: 599 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017241142\n",
      "\t Val. Loss: 0.0016990679\n",
      "Epoch: 600 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017125940\n",
      "\t Val. Loss: 0.0016990806\n",
      "Epoch: 601 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017254653\n",
      "\t Val. Loss: 0.0016991751\n",
      "Epoch: 602 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017428227\n",
      "\t Val. Loss: 0.0016985115\n",
      "Epoch: 603 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017258369\n",
      "\t Val. Loss: 0.0016984821\n",
      "Epoch: 604 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017238457\n",
      "\t Val. Loss: 0.0016986731\n",
      "Epoch: 605 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017257450\n",
      "\t Val. Loss: 0.0016982964\n",
      "Epoch: 606 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017227355\n",
      "\t Val. Loss: 0.0016982394\n",
      "Epoch: 607 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017380859\n",
      "\t Val. Loss: 0.0016974155\n",
      "Epoch: 608 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017239731\n",
      "\t Val. Loss: 0.0016974745\n",
      "Epoch: 609 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017329428\n",
      "\t Val. Loss: 0.0016975318\n",
      "Epoch: 610 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017546061\n",
      "\t Val. Loss: 0.0016979157\n",
      "Epoch: 611 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017479735\n",
      "\t Val. Loss: 0.0016976264\n",
      "Epoch: 612 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017365693\n",
      "\t Val. Loss: 0.0016976403\n",
      "Epoch: 613 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017259098\n",
      "\t Val. Loss: 0.0016979949\n",
      "Epoch: 614 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017491002\n",
      "\t Val. Loss: 0.0016982807\n",
      "Epoch: 615 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017324984\n",
      "\t Val. Loss: 0.0016994834\n",
      "Epoch: 616 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017467680\n",
      "\t Val. Loss: 0.0017001063\n",
      "Epoch: 617 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017191600\n",
      "\t Val. Loss: 0.0016998428\n",
      "Epoch: 618 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017405705\n",
      "\t Val. Loss: 0.0016993469\n",
      "Epoch: 619 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017307456\n",
      "\t Val. Loss: 0.0016993792\n",
      "Epoch: 620 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017727526\n",
      "\t Val. Loss: 0.0016989420\n",
      "Epoch: 621 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017334568\n",
      "\t Val. Loss: 0.0017001065\n",
      "Epoch: 622 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017304462\n",
      "\t Val. Loss: 0.0016992639\n",
      "Epoch: 623 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017127461\n",
      "\t Val. Loss: 0.0016987755\n",
      "Epoch: 624 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017378124\n",
      "\t Val. Loss: 0.0016982549\n",
      "Epoch: 625 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017237317\n",
      "\t Val. Loss: 0.0016983328\n",
      "Epoch: 626 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017335022\n",
      "\t Val. Loss: 0.0016985425\n",
      "Epoch: 627 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017337485\n",
      "\t Val. Loss: 0.0016981265\n",
      "Epoch: 628 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017328225\n",
      "\t Val. Loss: 0.0016984772\n",
      "Epoch: 629 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017276255\n",
      "\t Val. Loss: 0.0016985558\n",
      "Epoch: 630 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017311186\n",
      "\t Val. Loss: 0.0016993313\n",
      "Epoch: 631 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017172484\n",
      "\t Val. Loss: 0.0016995249\n",
      "Epoch: 632 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017019180\n",
      "\t Val. Loss: 0.0016990380\n",
      "Epoch: 633 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017269719\n",
      "\t Val. Loss: 0.0016988984\n",
      "Epoch: 634 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017236095\n",
      "\t Val. Loss: 0.0017002760\n",
      "Epoch: 635 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017254247\n",
      "\t Val. Loss: 0.0016996465\n",
      "Epoch: 636 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017328558\n",
      "\t Val. Loss: 0.0016994214\n",
      "Epoch: 637 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017175649\n",
      "\t Val. Loss: 0.0016985449\n",
      "Epoch: 638 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017001917\n",
      "\t Val. Loss: 0.0016989883\n",
      "Epoch: 639 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017124438\n",
      "\t Val. Loss: 0.0016977530\n",
      "Epoch: 640 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017672244\n",
      "\t Val. Loss: 0.0016984048\n",
      "Epoch: 641 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017298296\n",
      "\t Val. Loss: 0.0016985989\n",
      "Epoch: 642 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017277737\n",
      "\t Val. Loss: 0.0016985645\n",
      "Epoch: 643 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017867190\n",
      "\t Val. Loss: 0.0016996788\n",
      "Epoch: 644 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017264006\n",
      "\t Val. Loss: 0.0016994683\n",
      "Epoch: 645 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017024329\n",
      "\t Val. Loss: 0.0016993327\n",
      "Epoch: 646 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017298522\n",
      "\t Val. Loss: 0.0016986691\n",
      "Epoch: 647 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017163312\n",
      "\t Val. Loss: 0.0016979066\n",
      "Epoch: 648 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017273394\n",
      "\t Val. Loss: 0.0016970794\n",
      "Epoch: 649 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017413485\n",
      "\t Val. Loss: 0.0016973378\n",
      "Epoch: 650 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017342852\n",
      "\t Val. Loss: 0.0016974769\n",
      "Epoch: 651 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017226263\n",
      "\t Val. Loss: 0.0016975709\n",
      "Epoch: 652 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017212833\n",
      "\t Val. Loss: 0.0016971784\n",
      "Epoch: 653 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017352258\n",
      "\t Val. Loss: 0.0016966876\n",
      "Epoch: 654 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017151685\n",
      "\t Val. Loss: 0.0016961350\n",
      "Epoch: 655 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017311501\n",
      "\t Val. Loss: 0.0016965444\n",
      "Epoch: 656 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017123649\n",
      "\t Val. Loss: 0.0016963974\n",
      "Epoch: 657 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017169174\n",
      "\t Val. Loss: 0.0016955826\n",
      "Epoch: 658 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017247231\n",
      "\t Val. Loss: 0.0016957600\n",
      "Epoch: 659 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017177010\n",
      "\t Val. Loss: 0.0016958946\n",
      "Epoch: 660 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017417943\n",
      "\t Val. Loss: 0.0016966042\n",
      "Epoch: 661 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017115409\n",
      "\t Val. Loss: 0.0016975972\n",
      "Epoch: 662 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017070087\n",
      "\t Val. Loss: 0.0016975631\n",
      "Epoch: 663 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017001277\n",
      "\t Val. Loss: 0.0016972175\n",
      "Epoch: 664 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017186095\n",
      "\t Val. Loss: 0.0016975451\n",
      "Epoch: 665 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017269530\n",
      "\t Val. Loss: 0.0016973050\n",
      "Epoch: 666 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017185549\n",
      "\t Val. Loss: 0.0016978074\n",
      "Epoch: 667 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017248926\n",
      "\t Val. Loss: 0.0016986440\n",
      "Epoch: 668 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017370367\n",
      "\t Val. Loss: 0.0016985838\n",
      "Epoch: 669 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017196565\n",
      "\t Val. Loss: 0.0016980000\n",
      "Epoch: 670 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017170134\n",
      "\t Val. Loss: 0.0016986775\n",
      "Epoch: 671 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016938254\n",
      "\t Val. Loss: 0.0016981438\n",
      "Epoch: 672 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017346031\n",
      "\t Val. Loss: 0.0016982000\n",
      "Epoch: 673 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017161551\n",
      "\t Val. Loss: 0.0016979894\n",
      "Epoch: 674 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017152040\n",
      "\t Val. Loss: 0.0016970866\n",
      "Epoch: 675 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017342606\n",
      "\t Val. Loss: 0.0016976115\n",
      "Epoch: 676 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017481082\n",
      "\t Val. Loss: 0.0016977901\n",
      "Epoch: 677 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017333915\n",
      "\t Val. Loss: 0.0016980752\n",
      "Epoch: 678 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017209747\n",
      "\t Val. Loss: 0.0016984393\n",
      "Epoch: 679 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017443787\n",
      "\t Val. Loss: 0.0016983363\n",
      "Epoch: 680 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017200684\n",
      "\t Val. Loss: 0.0016985643\n",
      "Epoch: 681 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017230073\n",
      "\t Val. Loss: 0.0016991186\n",
      "Epoch: 682 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017261810\n",
      "\t Val. Loss: 0.0016987103\n",
      "Epoch: 683 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017513349\n",
      "\t Val. Loss: 0.0016987535\n",
      "Epoch: 684 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017317403\n",
      "\t Val. Loss: 0.0016989820\n",
      "Epoch: 685 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017678017\n",
      "\t Val. Loss: 0.0016991714\n",
      "Epoch: 686 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017219668\n",
      "\t Val. Loss: 0.0016985097\n",
      "Epoch: 687 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017708253\n",
      "\t Val. Loss: 0.0016986269\n",
      "Epoch: 688 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017068607\n",
      "\t Val. Loss: 0.0016975469\n",
      "Epoch: 689 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017325176\n",
      "\t Val. Loss: 0.0016978179\n",
      "Epoch: 690 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017114649\n",
      "\t Val. Loss: 0.0016970400\n",
      "Epoch: 691 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017096637\n",
      "\t Val. Loss: 0.0016968975\n",
      "Epoch: 692 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017124150\n",
      "\t Val. Loss: 0.0016967610\n",
      "Epoch: 693 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017403182\n",
      "\t Val. Loss: 0.0016968435\n",
      "Epoch: 694 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017431503\n",
      "\t Val. Loss: 0.0016968579\n",
      "Epoch: 695 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017297586\n",
      "\t Val. Loss: 0.0016976672\n",
      "Epoch: 696 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017402633\n",
      "\t Val. Loss: 0.0016977403\n",
      "Epoch: 697 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017383142\n",
      "\t Val. Loss: 0.0016975554\n",
      "Epoch: 698 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017338991\n",
      "\t Val. Loss: 0.0016974100\n",
      "Epoch: 699 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017561318\n",
      "\t Val. Loss: 0.0016972300\n",
      "Epoch: 700 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017351296\n",
      "\t Val. Loss: 0.0016971959\n",
      "Epoch: 701 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017059811\n",
      "\t Val. Loss: 0.0016964717\n",
      "Epoch: 702 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017137303\n",
      "\t Val. Loss: 0.0016960922\n",
      "Epoch: 703 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017274347\n",
      "\t Val. Loss: 0.0016965273\n",
      "Epoch: 704 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017504749\n",
      "\t Val. Loss: 0.0016967071\n",
      "Epoch: 705 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017353479\n",
      "\t Val. Loss: 0.0016965963\n",
      "Epoch: 706 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017459327\n",
      "\t Val. Loss: 0.0016965715\n",
      "Epoch: 707 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017182035\n",
      "\t Val. Loss: 0.0016964101\n",
      "Epoch: 708 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017134608\n",
      "\t Val. Loss: 0.0016962486\n",
      "Epoch: 709 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017146482\n",
      "\t Val. Loss: 0.0016964949\n",
      "Epoch: 710 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017884046\n",
      "\t Val. Loss: 0.0016962288\n",
      "Epoch: 711 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017069527\n",
      "\t Val. Loss: 0.0016960450\n",
      "Epoch: 712 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017742720\n",
      "\t Val. Loss: 0.0016958122\n",
      "Epoch: 713 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017232500\n",
      "\t Val. Loss: 0.0016956801\n",
      "Epoch: 714 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017199004\n",
      "\t Val. Loss: 0.0016967561\n",
      "Epoch: 715 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017137127\n",
      "\t Val. Loss: 0.0016956548\n",
      "Epoch: 716 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017345393\n",
      "\t Val. Loss: 0.0016964957\n",
      "Epoch: 717 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017296589\n",
      "\t Val. Loss: 0.0016965939\n",
      "Epoch: 718 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017003045\n",
      "\t Val. Loss: 0.0016958618\n",
      "Epoch: 719 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017453081\n",
      "\t Val. Loss: 0.0016954104\n",
      "Epoch: 720 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017209661\n",
      "\t Val. Loss: 0.0016960764\n",
      "Epoch: 721 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017385051\n",
      "\t Val. Loss: 0.0016959227\n",
      "Epoch: 722 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017147376\n",
      "\t Val. Loss: 0.0016959581\n",
      "Epoch: 723 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017580097\n",
      "\t Val. Loss: 0.0016965340\n",
      "Epoch: 724 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017249979\n",
      "\t Val. Loss: 0.0016967093\n",
      "Epoch: 725 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017255804\n",
      "\t Val. Loss: 0.0016963799\n",
      "Epoch: 726 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017113023\n",
      "\t Val. Loss: 0.0016962588\n",
      "Epoch: 727 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017365282\n",
      "\t Val. Loss: 0.0016963156\n",
      "Epoch: 728 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017383978\n",
      "\t Val. Loss: 0.0016963522\n",
      "Epoch: 729 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017307917\n",
      "\t Val. Loss: 0.0016960940\n",
      "Epoch: 730 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017266214\n",
      "\t Val. Loss: 0.0016956131\n",
      "Epoch: 731 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017209277\n",
      "\t Val. Loss: 0.0016951958\n",
      "Epoch: 732 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017128216\n",
      "\t Val. Loss: 0.0016940056\n",
      "Epoch: 733 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017130004\n",
      "\t Val. Loss: 0.0016937871\n",
      "Epoch: 734 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017122567\n",
      "\t Val. Loss: 0.0016936381\n",
      "Epoch: 735 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017338669\n",
      "\t Val. Loss: 0.0016935409\n",
      "Epoch: 736 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017145019\n",
      "\t Val. Loss: 0.0016944184\n",
      "Epoch: 737 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017124644\n",
      "\t Val. Loss: 0.0016943394\n",
      "Epoch: 738 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017326066\n",
      "\t Val. Loss: 0.0016945819\n",
      "Epoch: 739 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017588881\n",
      "\t Val. Loss: 0.0016954852\n",
      "Epoch: 740 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017377435\n",
      "\t Val. Loss: 0.0016956320\n",
      "Epoch: 741 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017507022\n",
      "\t Val. Loss: 0.0016955292\n",
      "Epoch: 742 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017216088\n",
      "\t Val. Loss: 0.0016954810\n",
      "Epoch: 743 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017139288\n",
      "\t Val. Loss: 0.0016956042\n",
      "Epoch: 744 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017023512\n",
      "\t Val. Loss: 0.0016953084\n",
      "Epoch: 745 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017139782\n",
      "\t Val. Loss: 0.0016949140\n",
      "Epoch: 746 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017161073\n",
      "\t Val. Loss: 0.0016958072\n",
      "Epoch: 747 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017462703\n",
      "\t Val. Loss: 0.0016957711\n",
      "Epoch: 748 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017121041\n",
      "\t Val. Loss: 0.0016949431\n",
      "Epoch: 749 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017215240\n",
      "\t Val. Loss: 0.0016957000\n",
      "Epoch: 750 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017309871\n",
      "\t Val. Loss: 0.0016965380\n",
      "Epoch: 751 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017266196\n",
      "\t Val. Loss: 0.0016974195\n",
      "Epoch: 752 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017098189\n",
      "\t Val. Loss: 0.0016972428\n",
      "Epoch: 753 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017201216\n",
      "\t Val. Loss: 0.0016967037\n",
      "Epoch: 754 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017020062\n",
      "\t Val. Loss: 0.0016965300\n",
      "Epoch: 755 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017158108\n",
      "\t Val. Loss: 0.0016962566\n",
      "Epoch: 756 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017054886\n",
      "\t Val. Loss: 0.0016961876\n",
      "Epoch: 757 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017285488\n",
      "\t Val. Loss: 0.0016967001\n",
      "Epoch: 758 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017331595\n",
      "\t Val. Loss: 0.0016970292\n",
      "Epoch: 759 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017487021\n",
      "\t Val. Loss: 0.0016964701\n",
      "Epoch: 760 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017234247\n",
      "\t Val. Loss: 0.0016970614\n",
      "Epoch: 761 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017211588\n",
      "\t Val. Loss: 0.0016963978\n",
      "Epoch: 762 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017311647\n",
      "\t Val. Loss: 0.0016964006\n",
      "Epoch: 763 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017122856\n",
      "\t Val. Loss: 0.0016962451\n",
      "Epoch: 764 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017228920\n",
      "\t Val. Loss: 0.0016964979\n",
      "Epoch: 765 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017261336\n",
      "\t Val. Loss: 0.0016961584\n",
      "Epoch: 766 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017856941\n",
      "\t Val. Loss: 0.0016963226\n",
      "Epoch: 767 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017206536\n",
      "\t Val. Loss: 0.0016964131\n",
      "Epoch: 768 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017129895\n",
      "\t Val. Loss: 0.0016960744\n",
      "Epoch: 769 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017199325\n",
      "\t Val. Loss: 0.0016957038\n",
      "Epoch: 770 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017484349\n",
      "\t Val. Loss: 0.0016956650\n",
      "Epoch: 771 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017191273\n",
      "\t Val. Loss: 0.0016958397\n",
      "Epoch: 772 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017037397\n",
      "\t Val. Loss: 0.0016956857\n",
      "Epoch: 773 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017576647\n",
      "\t Val. Loss: 0.0016958812\n",
      "Epoch: 774 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017286974\n",
      "\t Val. Loss: 0.0016952876\n",
      "Epoch: 775 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017167009\n",
      "\t Val. Loss: 0.0016952192\n",
      "Epoch: 776 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017323137\n",
      "\t Val. Loss: 0.0016957972\n",
      "Epoch: 777 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017252993\n",
      "\t Val. Loss: 0.0016968125\n",
      "Epoch: 778 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017349745\n",
      "\t Val. Loss: 0.0016978043\n",
      "Epoch: 779 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017205671\n",
      "\t Val. Loss: 0.0016984815\n",
      "Epoch: 780 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017175354\n",
      "\t Val. Loss: 0.0016983291\n",
      "Epoch: 781 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017176862\n",
      "\t Val. Loss: 0.0016985855\n",
      "Epoch: 782 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017185306\n",
      "\t Val. Loss: 0.0016987947\n",
      "Epoch: 783 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017057473\n",
      "\t Val. Loss: 0.0016975327\n",
      "Epoch: 784 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017037362\n",
      "\t Val. Loss: 0.0016963860\n",
      "Epoch: 785 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017129560\n",
      "\t Val. Loss: 0.0016956792\n",
      "Epoch: 786 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017082024\n",
      "\t Val. Loss: 0.0016961926\n",
      "Epoch: 787 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017183018\n",
      "\t Val. Loss: 0.0016969988\n",
      "Epoch: 788 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017291200\n",
      "\t Val. Loss: 0.0016974648\n",
      "Epoch: 789 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017137082\n",
      "\t Val. Loss: 0.0016972925\n",
      "Epoch: 790 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016990382\n",
      "\t Val. Loss: 0.0016971298\n",
      "Epoch: 791 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017230112\n",
      "\t Val. Loss: 0.0016965280\n",
      "Epoch: 792 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017176857\n",
      "\t Val. Loss: 0.0016969647\n",
      "Epoch: 793 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017229738\n",
      "\t Val. Loss: 0.0016960422\n",
      "Epoch: 794 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017285149\n",
      "\t Val. Loss: 0.0016970026\n",
      "Epoch: 795 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017130728\n",
      "\t Val. Loss: 0.0016974585\n",
      "Epoch: 796 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017285406\n",
      "\t Val. Loss: 0.0016980417\n",
      "Epoch: 797 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017298111\n",
      "\t Val. Loss: 0.0016979260\n",
      "Epoch: 798 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017228194\n",
      "\t Val. Loss: 0.0016971595\n",
      "Epoch: 799 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017223764\n",
      "\t Val. Loss: 0.0016962851\n",
      "Epoch: 800 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017235279\n",
      "\t Val. Loss: 0.0016960410\n",
      "Epoch: 801 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017003691\n",
      "\t Val. Loss: 0.0016958705\n",
      "Epoch: 802 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017380727\n",
      "\t Val. Loss: 0.0016959666\n",
      "Epoch: 803 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017038923\n",
      "\t Val. Loss: 0.0016957316\n",
      "Epoch: 804 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017170436\n",
      "\t Val. Loss: 0.0016952733\n",
      "Epoch: 805 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017281317\n",
      "\t Val. Loss: 0.0016951922\n",
      "Epoch: 806 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017133899\n",
      "\t Val. Loss: 0.0016951413\n",
      "Epoch: 807 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017098734\n",
      "\t Val. Loss: 0.0016957696\n",
      "Epoch: 808 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017234003\n",
      "\t Val. Loss: 0.0016961788\n",
      "Epoch: 809 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017358894\n",
      "\t Val. Loss: 0.0016966192\n",
      "Epoch: 810 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017204169\n",
      "\t Val. Loss: 0.0016961919\n",
      "Epoch: 811 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017176350\n",
      "\t Val. Loss: 0.0016965987\n",
      "Epoch: 812 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017051062\n",
      "\t Val. Loss: 0.0016965223\n",
      "Epoch: 813 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017354605\n",
      "\t Val. Loss: 0.0016964533\n",
      "Epoch: 814 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017587139\n",
      "\t Val. Loss: 0.0016965281\n",
      "Epoch: 815 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017333039\n",
      "\t Val. Loss: 0.0016962641\n",
      "Epoch: 816 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017315305\n",
      "\t Val. Loss: 0.0016962113\n",
      "Epoch: 817 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017293311\n",
      "\t Val. Loss: 0.0016969861\n",
      "Epoch: 818 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017049579\n",
      "\t Val. Loss: 0.0016966805\n",
      "Epoch: 819 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016913835\n",
      "\t Val. Loss: 0.0016961513\n",
      "Epoch: 820 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017164644\n",
      "\t Val. Loss: 0.0016958627\n",
      "Epoch: 821 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017008647\n",
      "\t Val. Loss: 0.0016952916\n",
      "Epoch: 822 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017266569\n",
      "\t Val. Loss: 0.0016948133\n",
      "Epoch: 823 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017266466\n",
      "\t Val. Loss: 0.0016951957\n",
      "Epoch: 824 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017334052\n",
      "\t Val. Loss: 0.0016952960\n",
      "Epoch: 825 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017230736\n",
      "\t Val. Loss: 0.0016953249\n",
      "Epoch: 826 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017125736\n",
      "\t Val. Loss: 0.0016951972\n",
      "Epoch: 827 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017424748\n",
      "\t Val. Loss: 0.0016956178\n",
      "Epoch: 828 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017031670\n",
      "\t Val. Loss: 0.0016956139\n",
      "Epoch: 829 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017160104\n",
      "\t Val. Loss: 0.0016955583\n",
      "Epoch: 830 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017319587\n",
      "\t Val. Loss: 0.0016952898\n",
      "Epoch: 831 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017269490\n",
      "\t Val. Loss: 0.0016948553\n",
      "Epoch: 832 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017165645\n",
      "\t Val. Loss: 0.0016952245\n",
      "Epoch: 833 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017222504\n",
      "\t Val. Loss: 0.0016948684\n",
      "Epoch: 834 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017234245\n",
      "\t Val. Loss: 0.0016950032\n",
      "Epoch: 835 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017117723\n",
      "\t Val. Loss: 0.0016958039\n",
      "Epoch: 836 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017193125\n",
      "\t Val. Loss: 0.0016963527\n",
      "Epoch: 837 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017224997\n",
      "\t Val. Loss: 0.0016965122\n",
      "Epoch: 838 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017318405\n",
      "\t Val. Loss: 0.0016967596\n",
      "Epoch: 839 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017306342\n",
      "\t Val. Loss: 0.0016972457\n",
      "Epoch: 840 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017083192\n",
      "\t Val. Loss: 0.0016965470\n",
      "Epoch: 841 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017292103\n",
      "\t Val. Loss: 0.0016964206\n",
      "Epoch: 842 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016878628\n",
      "\t Val. Loss: 0.0016957865\n",
      "Epoch: 843 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017412526\n",
      "\t Val. Loss: 0.0016956885\n",
      "Epoch: 844 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017186399\n",
      "\t Val. Loss: 0.0016959161\n",
      "Epoch: 845 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017153125\n",
      "\t Val. Loss: 0.0016959703\n",
      "Epoch: 846 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017122151\n",
      "\t Val. Loss: 0.0016957870\n",
      "Epoch: 847 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017414655\n",
      "\t Val. Loss: 0.0016956587\n",
      "Epoch: 848 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017329119\n",
      "\t Val. Loss: 0.0016955498\n",
      "Epoch: 849 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017370880\n",
      "\t Val. Loss: 0.0016962665\n",
      "Epoch: 850 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017046588\n",
      "\t Val. Loss: 0.0016962952\n",
      "Epoch: 851 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017039025\n",
      "\t Val. Loss: 0.0016954131\n",
      "Epoch: 852 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017321422\n",
      "\t Val. Loss: 0.0016957840\n",
      "Epoch: 853 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017259663\n",
      "\t Val. Loss: 0.0016963323\n",
      "Epoch: 854 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017206887\n",
      "\t Val. Loss: 0.0016962993\n",
      "Epoch: 855 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017568107\n",
      "\t Val. Loss: 0.0016961973\n",
      "Epoch: 856 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017335845\n",
      "\t Val. Loss: 0.0016963045\n",
      "Epoch: 857 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017237821\n",
      "\t Val. Loss: 0.0016949310\n",
      "Epoch: 858 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017028864\n",
      "\t Val. Loss: 0.0016943793\n",
      "Epoch: 859 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017205794\n",
      "\t Val. Loss: 0.0016951968\n",
      "Epoch: 860 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017059763\n",
      "\t Val. Loss: 0.0016961313\n",
      "Epoch: 861 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017154360\n",
      "\t Val. Loss: 0.0016962591\n",
      "Epoch: 862 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017014242\n",
      "\t Val. Loss: 0.0016959087\n",
      "Epoch: 863 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017175021\n",
      "\t Val. Loss: 0.0016956730\n",
      "Epoch: 864 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017209054\n",
      "\t Val. Loss: 0.0016955515\n",
      "Epoch: 865 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016994187\n",
      "\t Val. Loss: 0.0016952319\n",
      "Epoch: 866 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017158879\n",
      "\t Val. Loss: 0.0016949487\n",
      "Epoch: 867 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017179967\n",
      "\t Val. Loss: 0.0016951220\n",
      "Epoch: 868 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017501294\n",
      "\t Val. Loss: 0.0016947507\n",
      "Epoch: 869 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017333414\n",
      "\t Val. Loss: 0.0016944781\n",
      "Epoch: 870 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017207218\n",
      "\t Val. Loss: 0.0016948420\n",
      "Epoch: 871 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017239969\n",
      "\t Val. Loss: 0.0016950390\n",
      "Epoch: 872 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017306308\n",
      "\t Val. Loss: 0.0016946155\n",
      "Epoch: 873 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017135916\n",
      "\t Val. Loss: 0.0016948506\n",
      "Epoch: 874 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016915048\n",
      "\t Val. Loss: 0.0016947824\n",
      "Epoch: 875 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017137585\n",
      "\t Val. Loss: 0.0016946924\n",
      "Epoch: 876 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017297023\n",
      "\t Val. Loss: 0.0016951064\n",
      "Epoch: 877 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017315328\n",
      "\t Val. Loss: 0.0016956742\n",
      "Epoch: 878 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016906668\n",
      "\t Val. Loss: 0.0016959743\n",
      "Epoch: 879 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017445809\n",
      "\t Val. Loss: 0.0016955472\n",
      "Epoch: 880 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017078926\n",
      "\t Val. Loss: 0.0016955559\n",
      "Epoch: 881 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017207324\n",
      "\t Val. Loss: 0.0016957642\n",
      "Epoch: 882 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017299147\n",
      "\t Val. Loss: 0.0016952041\n",
      "Epoch: 883 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017150422\n",
      "\t Val. Loss: 0.0016946564\n",
      "Epoch: 884 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017203862\n",
      "\t Val. Loss: 0.0016944990\n",
      "Epoch: 885 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016850697\n",
      "\t Val. Loss: 0.0016939932\n",
      "Epoch: 886 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017225848\n",
      "\t Val. Loss: 0.0016936741\n",
      "Epoch: 887 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017293797\n",
      "\t Val. Loss: 0.0016943336\n",
      "Epoch: 888 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017315545\n",
      "\t Val. Loss: 0.0016948339\n",
      "Epoch: 889 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017149196\n",
      "\t Val. Loss: 0.0016948056\n",
      "Epoch: 890 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017009831\n",
      "\t Val. Loss: 0.0016943268\n",
      "Epoch: 891 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017129954\n",
      "\t Val. Loss: 0.0016938534\n",
      "Epoch: 892 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017239041\n",
      "\t Val. Loss: 0.0016942500\n",
      "Epoch: 893 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017008933\n",
      "\t Val. Loss: 0.0016949153\n",
      "Epoch: 894 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017200198\n",
      "\t Val. Loss: 0.0016940973\n",
      "Epoch: 895 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017428649\n",
      "\t Val. Loss: 0.0016938249\n",
      "Epoch: 896 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017302527\n",
      "\t Val. Loss: 0.0016937874\n",
      "Epoch: 897 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017591462\n",
      "\t Val. Loss: 0.0016939207\n",
      "Epoch: 898 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017072101\n",
      "\t Val. Loss: 0.0016937124\n",
      "Epoch: 899 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017419981\n",
      "\t Val. Loss: 0.0016942978\n",
      "Epoch: 900 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017240490\n",
      "\t Val. Loss: 0.0016943705\n",
      "Epoch: 901 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017204093\n",
      "\t Val. Loss: 0.0016944534\n",
      "Epoch: 902 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017103816\n",
      "\t Val. Loss: 0.0016940124\n",
      "Epoch: 903 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017037434\n",
      "\t Val. Loss: 0.0016946584\n",
      "Epoch: 904 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017293180\n",
      "\t Val. Loss: 0.0016951308\n",
      "Epoch: 905 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017465975\n",
      "\t Val. Loss: 0.0016939378\n",
      "Epoch: 906 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017139716\n",
      "\t Val. Loss: 0.0016936398\n",
      "Epoch: 907 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017170276\n",
      "\t Val. Loss: 0.0016939483\n",
      "Epoch: 908 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017136551\n",
      "\t Val. Loss: 0.0016935677\n",
      "Epoch: 909 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017258556\n",
      "\t Val. Loss: 0.0016937056\n",
      "Epoch: 910 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017039219\n",
      "\t Val. Loss: 0.0016946881\n",
      "Epoch: 911 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017506971\n",
      "\t Val. Loss: 0.0016948864\n",
      "Epoch: 912 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017281882\n",
      "\t Val. Loss: 0.0016952320\n",
      "Epoch: 913 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017088351\n",
      "\t Val. Loss: 0.0016951546\n",
      "Epoch: 914 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017648308\n",
      "\t Val. Loss: 0.0016952303\n",
      "Epoch: 915 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017435378\n",
      "\t Val. Loss: 0.0016956963\n",
      "Epoch: 916 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017440507\n",
      "\t Val. Loss: 0.0016948041\n",
      "Epoch: 917 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017159440\n",
      "\t Val. Loss: 0.0016943845\n",
      "Epoch: 918 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017124500\n",
      "\t Val. Loss: 0.0016945279\n",
      "Epoch: 919 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017076796\n",
      "\t Val. Loss: 0.0016940617\n",
      "Epoch: 920 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017206772\n",
      "\t Val. Loss: 0.0016944743\n",
      "Epoch: 921 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016868072\n",
      "\t Val. Loss: 0.0016937634\n",
      "Epoch: 922 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017599901\n",
      "\t Val. Loss: 0.0016937878\n",
      "Epoch: 923 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017178661\n",
      "\t Val. Loss: 0.0016940773\n",
      "Epoch: 924 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017016125\n",
      "\t Val. Loss: 0.0016941583\n",
      "Epoch: 925 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017201275\n",
      "\t Val. Loss: 0.0016941463\n",
      "Epoch: 926 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0018037709\n",
      "\t Val. Loss: 0.0016948122\n",
      "Epoch: 927 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017118089\n",
      "\t Val. Loss: 0.0016941614\n",
      "Epoch: 928 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017089870\n",
      "\t Val. Loss: 0.0016940730\n",
      "Epoch: 929 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017193070\n",
      "\t Val. Loss: 0.0016936502\n",
      "Epoch: 930 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016993374\n",
      "\t Val. Loss: 0.0016935474\n",
      "Epoch: 931 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016954148\n",
      "\t Val. Loss: 0.0016936909\n",
      "Epoch: 932 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016906424\n",
      "\t Val. Loss: 0.0016939014\n",
      "Epoch: 933 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017151234\n",
      "\t Val. Loss: 0.0016936218\n",
      "Epoch: 934 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017181461\n",
      "\t Val. Loss: 0.0016935978\n",
      "Epoch: 935 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017321615\n",
      "\t Val. Loss: 0.0016943173\n",
      "Epoch: 936 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016982586\n",
      "\t Val. Loss: 0.0016938101\n",
      "Epoch: 937 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017147427\n",
      "\t Val. Loss: 0.0016937783\n",
      "Epoch: 938 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017153924\n",
      "\t Val. Loss: 0.0016930987\n",
      "Epoch: 939 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017095983\n",
      "\t Val. Loss: 0.0016940337\n",
      "Epoch: 940 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017206683\n",
      "\t Val. Loss: 0.0016945471\n",
      "Epoch: 941 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017196412\n",
      "\t Val. Loss: 0.0016947733\n",
      "Epoch: 942 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017017516\n",
      "\t Val. Loss: 0.0016950351\n",
      "Epoch: 943 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017374261\n",
      "\t Val. Loss: 0.0016948379\n",
      "Epoch: 944 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017316866\n",
      "\t Val. Loss: 0.0016943837\n",
      "Epoch: 945 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017038038\n",
      "\t Val. Loss: 0.0016943516\n",
      "Epoch: 946 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017128611\n",
      "\t Val. Loss: 0.0016940212\n",
      "Epoch: 947 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017282115\n",
      "\t Val. Loss: 0.0016946709\n",
      "Epoch: 948 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016990971\n",
      "\t Val. Loss: 0.0016949315\n",
      "Epoch: 949 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017089108\n",
      "\t Val. Loss: 0.0016946920\n",
      "Epoch: 950 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016967322\n",
      "\t Val. Loss: 0.0016942491\n",
      "Epoch: 951 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017019993\n",
      "\t Val. Loss: 0.0016943265\n",
      "Epoch: 952 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016941302\n",
      "\t Val. Loss: 0.0016938096\n",
      "Epoch: 953 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017324950\n",
      "\t Val. Loss: 0.0016929432\n",
      "Epoch: 954 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016830202\n",
      "\t Val. Loss: 0.0016922565\n",
      "Epoch: 955 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017115150\n",
      "\t Val. Loss: 0.0016921119\n",
      "Epoch: 956 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017034152\n",
      "\t Val. Loss: 0.0016923939\n",
      "Epoch: 957 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017320503\n",
      "\t Val. Loss: 0.0016930426\n",
      "Epoch: 958 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017172977\n",
      "\t Val. Loss: 0.0016921962\n",
      "Epoch: 959 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017143643\n",
      "\t Val. Loss: 0.0016922361\n",
      "Epoch: 960 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017193405\n",
      "\t Val. Loss: 0.0016927911\n",
      "Epoch: 961 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017020871\n",
      "\t Val. Loss: 0.0016927100\n",
      "Epoch: 962 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017041994\n",
      "\t Val. Loss: 0.0016927982\n",
      "Epoch: 963 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016819768\n",
      "\t Val. Loss: 0.0016920386\n",
      "Epoch: 964 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017240936\n",
      "\t Val. Loss: 0.0016922962\n",
      "Epoch: 965 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017212251\n",
      "\t Val. Loss: 0.0016929033\n",
      "Epoch: 966 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017219712\n",
      "\t Val. Loss: 0.0016927452\n",
      "Epoch: 967 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016982808\n",
      "\t Val. Loss: 0.0016934179\n",
      "Epoch: 968 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016862887\n",
      "\t Val. Loss: 0.0016931886\n",
      "Epoch: 969 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017196323\n",
      "\t Val. Loss: 0.0016926374\n",
      "Epoch: 970 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017258053\n",
      "\t Val. Loss: 0.0016927682\n",
      "Epoch: 971 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017127871\n",
      "\t Val. Loss: 0.0016932200\n",
      "Epoch: 972 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017001076\n",
      "\t Val. Loss: 0.0016936649\n",
      "Epoch: 973 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017021740\n",
      "\t Val. Loss: 0.0016933297\n",
      "Epoch: 974 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017173136\n",
      "\t Val. Loss: 0.0016941901\n",
      "Epoch: 975 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017263732\n",
      "\t Val. Loss: 0.0016937477\n",
      "Epoch: 976 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017166690\n",
      "\t Val. Loss: 0.0016938922\n",
      "Epoch: 977 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017043088\n",
      "\t Val. Loss: 0.0016937120\n",
      "Epoch: 978 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017170479\n",
      "\t Val. Loss: 0.0016933415\n",
      "Epoch: 979 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017123667\n",
      "\t Val. Loss: 0.0016927285\n",
      "Epoch: 980 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017146649\n",
      "\t Val. Loss: 0.0016922071\n",
      "Epoch: 981 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017259888\n",
      "\t Val. Loss: 0.0016928836\n",
      "Epoch: 982 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017121513\n",
      "\t Val. Loss: 0.0016923260\n",
      "Epoch: 983 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017109772\n",
      "\t Val. Loss: 0.0016926726\n",
      "Epoch: 984 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017075198\n",
      "\t Val. Loss: 0.0016920160\n",
      "Epoch: 985 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017454244\n",
      "\t Val. Loss: 0.0016920291\n",
      "Epoch: 986 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017255062\n",
      "\t Val. Loss: 0.0016921655\n",
      "Epoch: 987 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017000230\n",
      "\t Val. Loss: 0.0016920762\n",
      "Epoch: 988 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017046173\n",
      "\t Val. Loss: 0.0016919893\n",
      "Epoch: 989 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016933900\n",
      "\t Val. Loss: 0.0016924476\n",
      "Epoch: 990 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017293007\n",
      "\t Val. Loss: 0.0016921034\n",
      "Epoch: 991 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017115864\n",
      "\t Val. Loss: 0.0016923889\n",
      "Epoch: 992 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017046413\n",
      "\t Val. Loss: 0.0016926115\n",
      "Epoch: 993 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017110538\n",
      "\t Val. Loss: 0.0016928682\n",
      "Epoch: 994 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017026750\n",
      "\t Val. Loss: 0.0016926102\n",
      "Epoch: 995 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0016948124\n",
      "\t Val. Loss: 0.0016927561\n",
      "Epoch: 996 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017153168\n",
      "\t Val. Loss: 0.0016936615\n",
      "Epoch: 997 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017108587\n",
      "\t Val. Loss: 0.0016931562\n",
      "Epoch: 998 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017093361\n",
      "\t Val. Loss: 0.0016930437\n",
      "Epoch: 999 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017047196\n",
      "\t Val. Loss: 0.0016929283\n",
      "Epoch: 1000 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.0017145546\n",
      "\t Val. Loss: 0.0016933823\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1000\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # print('training...')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, loss_function)\n",
    "    valid_loss = evaluate(model, test_loader, loss_function)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.10f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.10f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for X, y in test_loader:\n",
    "\n",
    "        model.hidden_cell = (torch.zeros(2, X.shape[0], model.hidden_layer_size).cuda(), torch.zeros(2, X.shape[0], model.hidden_layer_size).cuda())\n",
    "\n",
    "        predictions.append(model(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.transpose(torch.vstack(predictions), 0, 1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000.0, 52000.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(predictions[0])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(df_all[\"Value\"][-len(predictions[0]) - 1:].values)\n",
    "plt.plot((1 + predictions[0]) * df_all[\"Value\"][-len(predictions[0]) - 1: -1].values)\n",
    "plt.xlim(left=330)\n",
    "plt.ylim(42000, 52000)\n",
    "# df_tmp = df_test.iloc[SEQUENCE_LENGTH:].copy()\n",
    "# df_tmp[\"Bitcoin % Change (Predicted)\"] = predictions[0]\n",
    "# df_tmp[\"Value (Predicted)\"] = (1 + predictions[0]) * df_all[\"Value\"][-len(predictions[0]) - 1: -1].values\n",
    "# df_tmp[\"Value\"] = df_all[\"Value\"][-len(predictions[0]):]\n",
    "# df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for X, y in train_loader:\n",
    "\n",
    "        model.hidden_cell = (torch.zeros(2, X.shape[0], model.hidden_layer_size).cuda(\n",
    "        ), torch.zeros(2, X.shape[0], model.hidden_layer_size).cuda())\n",
    "\n",
    "        predictions.append(model(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.transpose(torch.vstack(predictions), 0, 1).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/zhenghongyi/Library/CloudStorage/OneDrive-ä¸ªäºº/NYU/Activities & Organizations/MCM-ICM/2022/MCM_2022_LSTM.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zhenghongyi/Library/CloudStorage/OneDrive-%E4%B8%AA%E4%BA%BA/NYU/Activities%20%26%20Organizations/MCM-ICM/2022/MCM_2022_LSTM.ipynb#ch0000021?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(df_all[\u001b[39m\"\u001b[39m\u001b[39mValue\u001b[39m\u001b[39m\"\u001b[39m][SEQUENCE_LENGTH:\u001b[39mlen\u001b[39m(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhenghongyi/Library/CloudStorage/OneDrive-%E4%B8%AA%E4%BA%BA/NYU/Activities%20%26%20Organizations/MCM-ICM/2022/MCM_2022_LSTM.ipynb#ch0000021?line=1'>2</a>\u001b[0m     predictions[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m SEQUENCE_LENGTH]\u001b[39m.\u001b[39mvalues)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhenghongyi/Library/CloudStorage/OneDrive-%E4%B8%AA%E4%BA%BA/NYU/Activities%20%26%20Organizations/MCM-ICM/2022/MCM_2022_LSTM.ipynb#ch0000021?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot((\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m predictions[\u001b[39m0\u001b[39m]) \u001b[39m*\u001b[39m df_all[\u001b[39m\"\u001b[39m\u001b[39mValue\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhenghongyi/Library/CloudStorage/OneDrive-%E4%B8%AA%E4%BA%BA/NYU/Activities%20%26%20Organizations/MCM-ICM/2022/MCM_2022_LSTM.ipynb#ch0000021?line=3'>4</a>\u001b[0m          [SEQUENCE_LENGTH \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m: SEQUENCE_LENGTH \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(predictions[\u001b[39m0\u001b[39m]) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mvalues)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhenghongyi/Library/CloudStorage/OneDrive-%E4%B8%AA%E4%BA%BA/NYU/Activities%20%26%20Organizations/MCM-ICM/2022/MCM_2022_LSTM.ipynb#ch0000021?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mxlim(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m40\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(df_all[\"Value\"][SEQUENCE_LENGTH:len(\n",
    "    predictions[0]) + SEQUENCE_LENGTH].values)\n",
    "plt.plot((1 + predictions[0]) * df_all[\"Value\"]\n",
    "         [SEQUENCE_LENGTH - 1: SEQUENCE_LENGTH + len(predictions[0]) - 1].values)\n",
    "\n",
    "plt.xlim(-1, 40)\n",
    "plt.ylim(575, 800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
